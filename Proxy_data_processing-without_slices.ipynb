{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "This is not the version of the code which is used to make the csv files.\n",
    "\n",
    "The results have been generated on data which has *not* been divided into slices and have *not* been interpolated.\n",
    "\n",
    "The calculations are only done on the peak Holocene and LIG data\n",
    "\n",
    "This notebook is for the confidence interval evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from netCDF4 import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable  \n",
    "import matplotlib\n",
    "import scipy\n",
    "import scipy.interpolate\n",
    "import sys\n",
    "sys.path.insert(0, '/srv/ccrc/data06/z5145948/Python/python_from_R/Holocene/sampled_models/plotting_files/')\n",
    "from plott import plott\n",
    "import scipy.interpolate\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "matplotlib.use('agg')\n",
    "from Cross_section import Cross_section\n",
    "from Proxy_graph_masked import Proxy_graph\n",
    "import Config\n",
    "#from Map_plot import Map_plot\n",
    "from collections import Counter\n",
    "import scipy.stats as stats\n",
    "from pylab import *\n",
    "rcParams['legend.numpoints'] = 1\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import time\n",
    "from IPython import embed\n",
    "import os\n",
    "import ast\n",
    "from matplotlib import gridspec\n",
    "\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "matplotlib.rcParams['text.latex.preamble'] = [\n",
    "    r'\\usepackage{wasysym}',\n",
    "    r'\\usepackage{textcomp}']\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 1200\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standardised latitudes and depths\n",
    "lat_standardised = np.genfromtxt('/srv/ccrc/data06/z5145948/Moving_water_mass/Data/latitude_levels.csv', delimiter=',')\n",
    "dep_standardised = np.genfromtxt('/srv/ccrc/data06/z5145948/Moving_water_mass/Data/depth_levels.csv', delimiter=',')\n",
    "\n",
    "# File saving location\n",
    "overleaf_loc = '/home/z5145948/Dropbox/Apps/Overleaf/Paper_2/Figures'\n",
    "\n",
    "# Set universal dividing depth\n",
    "dividing_depth = 2500\n",
    "\n",
    "# Set colors\n",
    "alpha = 0.5\n",
    "point_color_deep = (0,0,1,alpha)#'blue'\n",
    "point_color_shallow = (1,0,0,alpha)#'red'\n",
    "line_color_deep = 'cyan'\n",
    "line_color_shallow = 'magenta'\n",
    "\n",
    "# Font size\n",
    "fontsize = 20\n",
    "\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : fontsize}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "# Separate into different areas\n",
    "point_size_power = 1\n",
    "points_size_multiplier = 10\n",
    "south_lat_max = -15\n",
    "north_lat_min = 15\n",
    "lw = 4\n",
    "edgewidth=0.5\n",
    "size = 40\n",
    "weighting = False\n",
    "\n",
    "slice_width = 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Wrangling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####3 reading data function\n",
    "\n",
    "def read_data(folder):\n",
    "\n",
    "    # Import simulation details (summary) as dataframe\n",
    "    summary = pd.read_csv(folder + '_summary.txt', delimiter = ' ')\n",
    "\n",
    "    # Import simulation outputs\n",
    "    fh = Dataset(folder + 'output.nc')\n",
    "    proxy_simulations = fh.variables['var1_1'][:] \n",
    "\n",
    "    # Import samples (proxy data)\n",
    "    samples = pd.read_csv( folder + '_samples.txt', delimiter = ' ')\n",
    "\n",
    "    # Drop all unnecessary rows in summary\n",
    "    summary = summary.drop(['type', 'Row.names', 'reps', 'model', 'dataset', 'filename', 'success.rate'], axis = 1)\n",
    "\n",
    "    # Convert run.no to netcdf file index\n",
    "    summary['run.no'] = summary['run.no'] - 1\n",
    "    samples['run.no'] = samples['run.no'] - 1\n",
    "    \n",
    "    return(proxy_simulations, samples, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for reading and including cores that are in the Oliver compilation but not in the Peterson data set\n",
    "\n",
    "def Oliver_cores(minn, maxx):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    folder_location = '/srv/ccrc/data06/z5145948/Moving_water_mass/Data/Core_files/'\n",
    "    file_mat = ['GeoB4403_2.txt',\n",
    "             'GeoB1028_5.txt',\n",
    "             'GeoB2109_1.txt',\n",
    "             'GeoB3801_6.txt',\n",
    "             'V22_38.txt',\n",
    "             'V28_56.txt',\n",
    "             'V27_20.txt',\n",
    "             'RC12_339.txt',\n",
    "             'V32_128.txt',\n",
    "             'GIK16772_1.txt',\n",
    "             'MD96_2080.txt',\n",
    "             'MD06_3018.txt',\n",
    "             'NEAP18K.txt',\n",
    "             'KNR140_37JPC.txt']\n",
    "\n",
    "\n",
    "    location = ['Atlantic,','Atlantic,','Atlantic,','Atlantic,','Atlantic,','Atlantic,','Atlantic,','Indian,','Pacific,','Atlantic,','Atlantic,','Pacific,','Atlantic,','Atlantic,']\n",
    "\n",
    "    oliver_data = []\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while i < len(file_mat):\n",
    "        with open(folder_location + file_mat[i]) as f:\n",
    "            for line in f:\n",
    "                oliver_data.append(location[i] + line)\n",
    "        i += 1\n",
    "\n",
    "    df = pd.DataFrame([sub.split(\",\") for sub in oliver_data])\n",
    "\n",
    "    df.columns = ['Location','Core','Lat','Lon','Dep','Core depth','age','Species','pl1','pl2','d18O benthic','d13C']\n",
    "\n",
    "    df = df[['Core','Location','Lat','Lon','Dep','d13C','age']]\n",
    "\n",
    "    df['d13C'] = [i.rstrip() for i in df['d13C']]\n",
    "\n",
    "    df = df[df['age'].astype(float) > float(minn)]\n",
    "    df = df[df['age'].astype(float) < float(maxx)]\n",
    "\n",
    "    ################################# other data                                                                                                                                                                                                                                  \n",
    "\n",
    "    file_mat = ['CH69_K09.txt',\n",
    "    'MD03_2664.txt',\n",
    "    'MD95_2042.txt',\n",
    "    'U1308.txt',\n",
    "    'ODP1063.txt']\n",
    "\n",
    "    locations = ['CH69_K09\\tAtlantic\\t41.75\\t-47.35\\t4100\\t',\n",
    "              'MD03_2664\\tAtlantic\\t57.439000\\t-48.605800\\t3442.0\\t',\n",
    "              'MD95_2042\\tAtlantic\\t37.799833\\t-10.166500\\t3146.0\\t',\n",
    "              'U1308\\tAtlantic\\t49.877760\\t-24.238110\\t3871.0\\t',\n",
    "              'ODP1063\\tAtlantic\\t33.683333\\t-57.616667\\t4584\\t']\n",
    "\n",
    "    other_data = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(file_mat):\n",
    "        with open(folder_location + file_mat[i]) as f:\n",
    "            for line in f:\n",
    "                other_data.append(locations[i]+line)\n",
    "        i += 1\n",
    "\n",
    "    df2 = pd.DataFrame([sub.split(\"\\t\") for sub in other_data])\n",
    "\n",
    "    df2.columns = ['Core','Location','Lat','Lon','Dep','Core Depth','age','d13C']\n",
    "\n",
    "    df2 = df2[['Core','Location','Lat','Lon','Dep','d13C','age']]\n",
    "\n",
    "    df2['d13C'] = [i.rstrip() for i in df2['d13C']]\n",
    "\n",
    "    df2 = df2[df2['age'].astype(float) > float(minn)]\n",
    "    df2 = df2[df2['age'].astype(float) < float(maxx)]\n",
    "\n",
    "    results = pd.concat([df, df2])\n",
    "\n",
    "    results = results[results['d13C'] != '']\n",
    "\n",
    "    results['d13C'] = results['d13C'].astype(float)\n",
    "    results['Lat'] = results['Lat'].astype(float)\n",
    "    results['Lon'] = results['Lon'].astype(float)\n",
    "    results['Dep'] = results['Dep'].astype(float)\n",
    "\n",
    "    return(results)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def pl_cores(minn, maxx):\n",
    "\n",
    "    names = ['Core', 'Location', 'Lat', 'Lon', 'Dep']\n",
    "\n",
    "    # Read in the data\n",
    "    indopac = pd.read_table(\"../Moving_water_mass/Data/Core_files/indopac_core_data_LS16.txt\", delimiter = ',', names = names)\n",
    "    atl = pd.read_table(\"../Moving_water_mass/Data/Core_files/atl_core_data_LS16.txt\", delimiter = ',', names = names)\n",
    "    add = pd.read_table(\"../Moving_water_mass/Data/Core_files/Additional_core_locations.txt\", delimiter = ',', usecols = [0, 1, 2, 3, 4], names = names)\n",
    "\n",
    "    # Join all into a single dataframe\n",
    "    df = indopac.append(atl)\n",
    "    df = df.append(add)\n",
    "    df = df.reset_index(drop = True)\n",
    "\n",
    "    df['d18O names'] = df['Core'] + '_ageLS16.txt'\n",
    "    df['d13C names'] = df['Core'] + '_d13C.txt'\n",
    "\n",
    "    # Loop over the dataset and interpolate each core\n",
    "    i = 0\n",
    "    results_dict = {}\n",
    "\n",
    "    while i < df.count()[0]:\n",
    "\n",
    "        try:\n",
    "            df_d18O = pd.read_table('../Moving_water_mass/Data/Core_files/' + df.loc[i]['d18O names'], delim_whitespace = True, names = ['depth', 'age'], skip_blank_lines = True, na_values = 'NAN')\n",
    "        except:\n",
    "            i += 1\n",
    "            continue\n",
    "        try:\n",
    "            df_d13C = pd.read_table('../Moving_water_mass/Data/Core_files/' + df.loc[i]['d13C names'], delim_whitespace = True, names = ['depth', 'd13C'], skip_blank_lines = True, na_values = 'NAN')\n",
    "        except:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        df_d18O = df_d18O.dropna(subset = ['age']) \n",
    "        df_d13C = df_d13C.dropna(subset = ['d13C'])\n",
    "\n",
    "        df_d18O = df_d18O.reset_index(drop = True)\n",
    "        df_d13C = df_d13C.reset_index(drop = True)\n",
    "\n",
    "        interp = scipy.interpolate.interp1d(df_d18O['depth'], df_d18O['age'], bounds_error = True)\n",
    "        try:\n",
    "            df_d13C['age'] = interp(df_d13C['depth'])\n",
    "        except:\n",
    "            try:\n",
    "                interp2 = scipy.interpolate.interp1d(df_d18O['depth'], df_d18O['age'], bounds_error = False)\n",
    "                df_d13C['age'] = interp2(df_d13C['depth'])\n",
    "            except:\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        df_d13C = df_d13C.dropna(subset = ['age'])\n",
    "        df_d13C = df_d13C.reset_index(drop = True)\n",
    "\n",
    "        df_d13C = df_d13C[df_d13C['age'] > minn]\n",
    "        df_d13C = df_d13C[df_d13C['age'] < maxx]\n",
    "\n",
    "        if len(df_d13C) > 0:\n",
    "            df_results = df.drop(['d18O names', 'd13C names'], axis = 1)\n",
    "            df_results = df_results.loc[df_results.index.repeat(len(df_d13C))].loc[[i]]\n",
    "\n",
    "            df_d13C = df_d13C.drop(['depth'], axis = 1)\n",
    "\n",
    "            df_results = df_results.reset_index(drop = True).join(df_d13C.reset_index(drop = True))\n",
    "            results_dict.update({\n",
    "                df_results.Core[0] : df_results.drop(['Core'], axis = 1)\n",
    "            })\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "    df_results = pd.concat(results_dict).reset_index()\n",
    "    df_results = df_results.rename(columns = {'level_0' : 'Core'})\n",
    "    df_results = df_results.drop(['level_1'], axis = 1)\n",
    "\n",
    "    return(df_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def slicing_data(time_min,time_max):\n",
    "    # ################## read in the oliver data using the predefined function\n",
    "\n",
    "    # Age range to look over\n",
    "\n",
    "    df_pl = pl_cores(time_min,time_max)\n",
    "\n",
    "    df_oliver = Oliver_cores(time_min,time_max)\n",
    "\n",
    "    ###################3 join the dataframes\n",
    "\n",
    "    df_results = df_pl.append(df_oliver)\n",
    "\n",
    "    #################### get only the atlantic cores\n",
    "\n",
    "    df = df_results.reset_index(drop = True)\n",
    "    df['Dep'] = abs(df['Dep'])\n",
    "\n",
    "    df_atl = df[df['Location'] == 'Atlantic']\n",
    "    df_atl = df_atl.reset_index(drop = True)\n",
    "    df_atl['age'] = df_atl.age.astype(float)\n",
    "        \n",
    "    return(df_atl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Atlantic_regions(proxy_compilation):\n",
    "    # Get volume weight average with time slices\n",
    "    interpolated_samples['weights'] = np.nan\n",
    "    interpolated_samples['regions'] = np.nan    \n",
    "\n",
    "    # NEA\n",
    "    interpolated_samples.loc[((interpolated_samples['Lon'] < 20) | (interpolated_samples['Lon'] > (-33))) & (interpolated_samples['Lat'] > 0.1),\n",
    "                            'weights'] = 4.3\n",
    "    interpolated_samples.loc[((interpolated_samples['Lon'] < 20) | (interpolated_samples['Lon'] > (-33))) & (interpolated_samples['Lat'] > 0.1),\n",
    "                            'regions'] = 'NEA'\n",
    "    \n",
    "\n",
    "    # NWA\n",
    "    interpolated_samples.loc[(interpolated_samples['Lon'] < (-33)) & (interpolated_samples['Lon'] > (-180)) & (interpolated_samples['Lat'] > 0.1),\n",
    "                             'weights'] = 4.9\n",
    "    interpolated_samples.loc[(interpolated_samples['Lon'] < (-33)) & (interpolated_samples['Lon'] > (-180)) & (interpolated_samples['Lat'] > 0.1),\n",
    "                             'regions'] = 'NWA'\n",
    "\n",
    "    # SEA\n",
    "    interpolated_samples.loc[((interpolated_samples['Lon'] < 30) | (interpolated_samples['Lon'] > (-14.6))) & (interpolated_samples['Lat'] < 0) & (interpolated_samples['Lat'] > -55),\n",
    "                             'weights'] = 3.5\n",
    "    interpolated_samples.loc[((interpolated_samples['Lon'] < 30) | (interpolated_samples['Lon'] > (-14.6))) & (interpolated_samples['Lat'] < 0) & (interpolated_samples['Lat'] > -55),\n",
    "                             'regions'] = 'SEA'\n",
    "    \n",
    "    # SA\n",
    "    interpolated_samples.loc[((interpolated_samples['Lon'] < 30) | (interpolated_samples['Lon'] > (-22))) & (interpolated_samples['Lat'] < -40) & (interpolated_samples['Lat'] > -55),\n",
    "                            'weights'] = 0.7\n",
    "    interpolated_samples.loc[((interpolated_samples['Lon'] < 30) | (interpolated_samples['Lon'] > (-22))) & (interpolated_samples['Lat'] < -40) & (interpolated_samples['Lat'] > -55),\n",
    "                            'regions'] = 'SA'    \n",
    "\n",
    "    # SWA\n",
    "    interpolated_samples.loc[(interpolated_samples['Lon'] > (-60)) & (interpolated_samples['Lon'] < (-14.6)) & (interpolated_samples['Lat'] < 0) & (interpolated_samples['Lat'] > -55),\n",
    "                            'weights'] = 5.0\n",
    "    interpolated_samples.loc[(interpolated_samples['Lon'] > (-60)) & (interpolated_samples['Lon'] < (-14.6)) & (interpolated_samples['Lat'] < 0) & (interpolated_samples['Lat'] > -55),\n",
    "                            'regions'] = 'SWA'    \n",
    "    \n",
    "    return(interpolated_samples)\n",
    "    \n",
    "def Pacific_regions(proxy_compilation):\n",
    "    # Get volume weight average with time slices\n",
    "    interpolated_samples['weights'] = np.nan\n",
    "    interpolated_samples['regions'] = np.nan      \n",
    "\n",
    "    # North\n",
    "    interpolated_samples.loc[(interpolated_samples['Lat'] > 0) ,'weights'] = 21.2\n",
    "    interpolated_samples.loc[(interpolated_samples['Lat'] > 0) ,'regions'] = 'NP'    \n",
    "\n",
    "    # South\n",
    "    interpolated_samples.loc[(interpolated_samples['Lat'] < 0) ,'weights'] = 23.9\n",
    "    interpolated_samples.loc[(interpolated_samples['Lat'] < 0) ,'regions'] = 'SP'\n",
    "    \n",
    "    return(interpolated_samples)\n",
    "\n",
    "def Indian_regions(proxy_compilation):\n",
    "    # Get volume weight average with time slices # Use a single region for the Indian Ocean\n",
    "    interpolated_samples['weights'] = 1\n",
    "    interpolated_samples['regions'] = 'I'\n",
    "    \n",
    "    return(interpolated_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averaging_by_region(proxy_compilation):\n",
    "    # group the cores based on the age and the region (indicated by weights)\n",
    "    grouped_by_age_region = proxy_compilation.groupby(['regions'])\n",
    "    \n",
    "    averaged_by_age_region = {}\n",
    "    stdev_by_age_region = {}\n",
    "    measurement_count = {}\n",
    "    core_count = {}\n",
    "    CI_by_age_region = {}\n",
    "    \n",
    "    for key, group in grouped_by_age_region:\n",
    "        \n",
    "        # find group means\n",
    "        averaged_by_age_region.update({\n",
    "            key: np.mean(group)\n",
    "        })\n",
    "        # find total number of cores\n",
    "        core_count.update({\n",
    "            key: len(group['count'].dropna())\n",
    "        }) \n",
    "        # find total measurement count\n",
    "        measurement_count.update({\n",
    "            key: np.nansum(group['count'])\n",
    "        })\n",
    "        # find the standard deviation across the slices and cores\n",
    "        stdev_by_age_region.update({\n",
    "            key: np.std(group['d13C'])\n",
    "        })\n",
    "        # express variation as a confidence interval\n",
    "        CI_by_age_region.update({\n",
    "            key: 1.96 * np.std(group['d13C'])/(np.sqrt(len(group['count'].dropna()))) #CI of 95%, 1.96 is zscore\n",
    "        })        \n",
    "        \n",
    "    # Convert dictionaries to dataframes\n",
    "    averaged_by_age_region = pd.concat(averaged_by_age_region,axis=1).T.reset_index(drop=False).rename({'level_1':'regions'},axis=1).drop('level_0',axis=1)\n",
    "    measurement_count = pd.DataFrame.from_dict(measurement_count,orient='index')\n",
    "    core_count = pd.DataFrame.from_dict(core_count,orient='index')\n",
    "    stdev_by_age_region = pd.DataFrame.from_dict(stdev_by_age_region,orient='index')\n",
    "    CI_by_age_region = pd.DataFrame.from_dict(CI_by_age_region,orient='index')    \n",
    "    \n",
    "    # Add columns of number of cores and number of measurements to the dataframe\n",
    "    averaged_by_age_region['measurement_count'] = list(measurement_count[0])\n",
    "    averaged_by_age_region['core_count'] = list(core_count[0])  \n",
    "    averaged_by_age_region['slice_stdev'] = list(stdev_by_age_region[0])\n",
    "    averaged_by_age_region['CI'] = list(CI_by_age_region[0])\n",
    "        \n",
    "    return(averaged_by_age_region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sort_data(time_min,time_max,region_func,interp_data_name,notinterp_data_name):\n",
    "\n",
    "    # Slice the data\n",
    "    proxy_compilation = slicing_data(time_min,time_max)\n",
    "\n",
    "    # get region weights\n",
    "    proxy_compilation = region_func(proxy_compilation)\n",
    "    \n",
    "    # Get regional average and ocean basin averages\n",
    "    averaged_by_age_region = averaging_by_region(proxy_compilation)\n",
    "\n",
    "    # drop weights (because I'm not using them now)\n",
    "    interpolated_samples = interpolated_samples.drop('weights',axis=1)\n",
    "    \n",
    "    return(averaged_by_age, averaged_by_age_region)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak LIG and peak Holocene time periods\n",
    "LIG_min = 120\n",
    "LIG_max = 125\n",
    "Hol_min = 4\n",
    "Hol_max = 7\n",
    "\n",
    "# set up other variables\n",
    "\n",
    "# Holocene, Atlantic\n",
    "Hol_atl_nointerp_data_name = \"Data/Atlantic_Holocene_profile_PL.csv\"\n",
    "Hol_atl_interp_data_name = \"/srv/ccrc/data06/z5145948/Moving_water_mass/Data/interpolated_Hol_Atlantic.csv\"\n",
    "\n",
    "# LIG, Atlantic\n",
    "LIG_atl_nointerp_data_name = \"Data/Atlantic_LIG_profile_PL.csv\"\n",
    "LIG_atl_interp_data_name = \"/srv/ccrc/data06/z5145948/Moving_water_mass/Data/interpolated_LIG_Atlantic.csv\"\n",
    "\n",
    "# Holocene, Pacific\n",
    "Hol_pac_nointerp_data_name = \"Data/Pacific_Holocene_profile_PL.csv\"\n",
    "Hol_pac_interp_data_name = \"/srv/ccrc/data06/z5145948/Moving_water_mass/Data/interpolated_Hol_Pacific.csv\"\n",
    "\n",
    "# LIG, Pacific\n",
    "LIG_pac_nointerp_data_name = \"Data/Pacific_LIG_profile_PL.csv\"\n",
    "LIG_pac_interp_data_name = \"/srv/ccrc/data06/z5145948/Moving_water_mass/Data/interpolated_LIG_Pacific.csv\"\n",
    "\n",
    "# Holocene, Indian\n",
    "Hol_ind_nointerp_data_name = \"Data/Indian_Holocene_profile_PL.csv\"\n",
    "Hol_ind_interp_data_name = \"/srv/ccrc/data06/z5145948/Moving_water_mass/Data/interpolated_Hol_Indian.csv\"\n",
    "\n",
    "# LIG, Indian\n",
    "LIG_ind_nointerp_data_name = \"Data/Indian_LIG_profile_PL.csv\"\n",
    "LIG_ind_interp_data_name = \"/srv/ccrc/data06/z5145948/Moving_water_mass/Data/interpolated_LIG_Indian.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'interpolated_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-25eab932c7ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Run function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     averaged_by_age, averaged_by_age_region, interpolated_samples = sort_data(\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtime_min\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime_max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mregion_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minterp_data_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnotinterp_data_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-5ee784cca8d7>\u001b[0m in \u001b[0;36msort_data\u001b[0;34m(time_min, time_max, region_func, interp_data_name, notinterp_data_name)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# get region weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mproxy_compilation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy_compilation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Get regional average and ocean basin averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-bcf4968efa83>\u001b[0m in \u001b[0;36mAtlantic_regions\u001b[0;34m(proxy_compilation)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mAtlantic_regions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy_compilation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Get volume weight average with time slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0minterpolated_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0minterpolated_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'regions'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'interpolated_samples' is not defined"
     ]
    }
   ],
   "source": [
    "# Run function for each scenario (three oceans, two time periods)\n",
    "\n",
    "interpolated_results_dict = {}\n",
    "average_age_interpolated_results_dict = {}\n",
    "average_age_region_interpolated_results_dict = {}\n",
    "\n",
    "for label,time_min,time_max,region_func,interp_data_name,notinterp_data_name in zip(\n",
    "    ['Hol_atl','LIG_atl','Hol_pac','LIG_pac','Hol_ind','LIG_ind'],\n",
    "    [Hol_min,LIG_min,Hol_min,LIG_min,Hol_min,LIG_min],\n",
    "    [Hol_max,LIG_max,Hol_max,LIG_max,Hol_max,LIG_max],\n",
    "    [Atlantic_regions,Atlantic_regions,Pacific_regions,Pacific_regions,Indian_regions,Indian_regions],\n",
    "    [Hol_atl_interp_data_name,LIG_atl_interp_data_name,Hol_pac_interp_data_name,LIG_pac_interp_data_name,Hol_ind_interp_data_name,LIG_ind_interp_data_name],\n",
    "    [Hol_atl_nointerp_data_name,LIG_atl_nointerp_data_name,Hol_pac_nointerp_data_name,LIG_pac_nointerp_data_name,Hol_ind_nointerp_data_name,LIG_ind_nointerp_data_name]):\n",
    "    \n",
    "    # Run function\n",
    "    averaged_by_age, averaged_by_age_region, interpolated_samples = sort_data(\n",
    "        time_min,time_max,region_func,interp_data_name,notinterp_data_name\n",
    "    )\n",
    "    \n",
    "    # Add results to dictionary\n",
    "    interpolated_results_dict.update({\n",
    "        label : interpolated_samples\n",
    "    })\n",
    "    average_age_interpolated_results_dict.update({\n",
    "        label : averaged_by_age\n",
    "    })   \n",
    "    average_age_region_interpolated_results_dict.update({\n",
    "        label : averaged_by_age_region\n",
    "    })     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create results dataframes from dictionaries\n",
    "df_interpolated_results = pd.concat(interpolated_results_dict).reset_index(drop=True)\n",
    "df_average_age_interpolated_results = pd.concat(average_age_interpolated_results_dict).reset_index().drop('level_1',axis=1).rename(columns=\n",
    "                                                                                 {'level_0':'location'})\n",
    "df_average_age_region_interpolated_results = pd.concat(average_age_region_interpolated_results_dict).reset_index().drop('level_1',axis=1).rename(columns=\n",
    "                                                                                 {'level_0':'location'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find average results of peak period\n",
    "df_LIG_peak = df_average_age_region_interpolated_results[(df_average_age_region_interpolated_results['age'] > LIG_peak_min) & (df_average_age_region_interpolated_results['age'] < LIG_peak_max)]\n",
    "df_Hol_peak = df_average_age_region_interpolated_results[(df_average_age_region_interpolated_results['age'] > Hol_peak_min) & (df_average_age_region_interpolated_results['age'] < Hol_peak_max)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Average over all time slices for LIG \n",
    "LIG_peak_averaged_dict = {}\n",
    "for index,group in df_LIG_peak.groupby(['location','regions']):\n",
    "    LIG_peak_averaged_dict.update({\n",
    "        index:pd.DataFrame(np.mean(group)).T\n",
    "    })\n",
    "df_LIG_peak_averaged = pd.concat(LIG_peak_averaged_dict)\n",
    "    \n",
    "# Average over all time slices for Holocene\n",
    "Hol_peak_averaged_dict = {}\n",
    "for index,group in df_Hol_peak.groupby(['location','regions']):\n",
    "    Hol_peak_averaged_dict.update({\n",
    "        index:pd.DataFrame(np.mean(group)).T\n",
    "    })    \n",
    "df_Hol_peak_averaged = pd.concat(Hol_peak_averaged_dict)    \n",
    "    \n",
    "# Format dataframes for merge\n",
    "df_Hol_peak_averaged = df_Hol_peak_averaged.reset_index().drop(['weights','age','count','level_2','level_0'],axis=1)\n",
    "df_Hol_peak_averaged = df_Hol_peak_averaged.rename(columns={'level_1':'Location'})\n",
    "df_Hol_peak_averaged.set_index(['Location'],inplace=True)\n",
    "\n",
    "df_LIG_peak_averaged = df_LIG_peak_averaged.reset_index().drop(['age','count','level_2','level_0'],axis=1)\n",
    "df_LIG_peak_averaged = df_LIG_peak_averaged.rename(columns={'level_1':'Location'})\n",
    "df_LIG_peak_averaged.set_index(['Location'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_Hol_peak_averaged.join(df_LIG_peak_averaged,lsuffix=' Holocene',rsuffix=' LIG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['d13C LIG'] - test['d13C Holocene']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax1 = subplot2grid((1,1),(0,0))\n",
    "ax1 = subplot2grid((1,1),(0,0))\n",
    "\n",
    "plt.scatter(test.index,test['d13C LIG'])\n",
    "plt.scatter(test.index,test['d13C Holocene'])\n",
    "\n",
    "plt.errorbar(test.index,test['d13C LIG'],yerr=test['CI LIG'],ls='none')\n",
    "plt.errorbar(test.index,test['d13C Holocene'],yerr=test['CI Holocene'],ls='none')\n",
    "\n",
    "plt.bar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
