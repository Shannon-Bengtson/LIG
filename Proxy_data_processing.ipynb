{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from netCDF4 import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable  \n",
    "import matplotlib\n",
    "import scipy\n",
    "import scipy.interpolate\n",
    "import sys\n",
    "sys.path.insert(0, '/srv/ccrc/data06/z5145948/Python/python_from_R/Holocene/sampled_models/plotting_files/')\n",
    "from plott import plott\n",
    "import scipy.interpolate\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "matplotlib.use('agg')\n",
    "from Cross_section import Cross_section\n",
    "from Proxy_graph_masked import Proxy_graph\n",
    "import Config\n",
    "#from Map_plot import Map_plot\n",
    "from collections import Counter\n",
    "import scipy.stats as stats\n",
    "from pylab import *\n",
    "rcParams['legend.numpoints'] = 1\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import time\n",
    "import os\n",
    "import ast\n",
    "from matplotlib import gridspec\n",
    "\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "matplotlib.rcParams['text.latex.preamble'] = [\n",
    "    r'\\usepackage{wasysym}',\n",
    "    r'\\usepackage{textcomp}']\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 1200\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standardised latitudes and depths\n",
    "lat_standardised = np.genfromtxt('/srv/ccrc/data06/z5145948/Moving_water_mass/Data/latitude_levels.csv', delimiter=',')\n",
    "dep_standardised = np.genfromtxt('/srv/ccrc/data06/z5145948/Moving_water_mass/Data/depth_levels.csv', delimiter=',')\n",
    "\n",
    "# File saving location\n",
    "overleaf_loc = '/home/z5145948/Dropbox/Apps/Overleaf/Paper_2/Figures'\n",
    "\n",
    "# Set universal dividing depth\n",
    "dividing_depth = 2500\n",
    "\n",
    "# Set colors\n",
    "alpha = 0.5\n",
    "point_color_deep = (0,0,1,alpha)#'blue'\n",
    "point_color_shallow = (1,0,0,alpha)#'red'\n",
    "line_color_deep = 'cyan'\n",
    "line_color_shallow = 'magenta'\n",
    "\n",
    "# Font size\n",
    "fontsize = 20\n",
    "\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : fontsize}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "# Separate into different areas\n",
    "point_size_power = 1\n",
    "points_size_multiplier = 10\n",
    "south_lat_max = -15\n",
    "north_lat_min = 15\n",
    "lw = 4\n",
    "edgewidth=0.5\n",
    "size = 40\n",
    "weighting = False\n",
    "\n",
    "slice_width = 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Wrangling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####3 reading data function\n",
    "\n",
    "def read_data(folder):\n",
    "\n",
    "    # Import simulation details (summary) as dataframe\n",
    "    summary = pd.read_csv(folder + '_summary.txt', delimiter = ' ')\n",
    "\n",
    "    # Import simulation outputs\n",
    "    fh = Dataset(folder + 'output.nc')\n",
    "    proxy_simulations = fh.variables['var1_1'][:] \n",
    "\n",
    "    # Import samples (proxy data)\n",
    "    samples = pd.read_csv( folder + '_samples.txt', delimiter = ' ')\n",
    "\n",
    "    # Drop all unnecessary rows in summary\n",
    "    summary = summary.drop(['type', 'Row.names', 'reps', 'model', 'dataset', 'filename', 'success.rate'], axis = 1)\n",
    "\n",
    "    # Convert run.no to netcdf file index\n",
    "    summary['run.no'] = summary['run.no'] - 1\n",
    "    samples['run.no'] = samples['run.no'] - 1\n",
    "    \n",
    "    return(proxy_simulations, samples, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for reading and including cores that are in the Oliver compilation but not in the Peterson data set\n",
    "\n",
    "def Oliver_cores(minn, maxx):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    folder_location = '/srv/ccrc/data06/z5145948/Moving_water_mass/Data/Core_files/'\n",
    "    file_mat = ['GeoB4403_2.txt',\n",
    "             'GeoB1028_5.txt',\n",
    "             'GeoB2109_1.txt',\n",
    "             'GeoB3801_6.txt',\n",
    "             'V22_38.txt',\n",
    "             'V28_56.txt',\n",
    "             'V27_20.txt',\n",
    "             'RC12_339.txt',\n",
    "             'V32_128.txt',\n",
    "             'GIK16772_1.txt',\n",
    "             'MD96_2080.txt',\n",
    "             'MD06_3018.txt',\n",
    "             'NEAP18K.txt',\n",
    "             'KNR140_37JPC.txt']\n",
    "\n",
    "\n",
    "    location = ['Atlantic,','Atlantic,','Atlantic,','Atlantic,','Atlantic,','Atlantic,','Atlantic,','Indian,','Pacific,','Atlantic,','Atlantic,','Pacific,','Atlantic,','Atlantic,']\n",
    "\n",
    "    oliver_data = []\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while i < len(file_mat):\n",
    "        with open(folder_location + file_mat[i]) as f:\n",
    "            for line in f:\n",
    "                oliver_data.append(location[i] + line)\n",
    "        i += 1\n",
    "\n",
    "    df = pd.DataFrame([sub.split(\",\") for sub in oliver_data])\n",
    "\n",
    "    df.columns = ['Location','Core','Lat','Lon','Dep','Core depth','age','Species','pl1','pl2','d18O benthic','d13C']\n",
    "\n",
    "    df = df[['Core','Location','Lat','Lon','Dep','d13C','age']]\n",
    "\n",
    "    df['d13C'] = [i.rstrip() for i in df['d13C']]\n",
    "\n",
    "    df = df[df['age'].astype(float) > float(minn)]\n",
    "    df = df[df['age'].astype(float) < float(maxx)]\n",
    "\n",
    "    ################################# other data                                                                                                                                                                                                                                  \n",
    "\n",
    "    file_mat = ['CH69_K09.txt',\n",
    "    'MD03_2664.txt',\n",
    "    'MD95_2042.txt',\n",
    "    'U1308.txt',\n",
    "    'ODP1063.txt']\n",
    "\n",
    "    locations = ['CH69_K09\\tAtlantic\\t41.75\\t-47.35\\t4100\\t',\n",
    "              'MD03_2664\\tAtlantic\\t57.439000\\t-48.605800\\t3442.0\\t',\n",
    "              'MD95_2042\\tAtlantic\\t37.799833\\t-10.166500\\t3146.0\\t',\n",
    "              'U1308\\tAtlantic\\t49.877760\\t-24.238110\\t3871.0\\t',\n",
    "              'ODP1063\\tAtlantic\\t33.683333\\t-57.616667\\t4584\\t']\n",
    "\n",
    "    other_data = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(file_mat):\n",
    "        with open(folder_location + file_mat[i]) as f:\n",
    "            for line in f:\n",
    "                other_data.append(locations[i]+line)\n",
    "        i += 1\n",
    "\n",
    "    df2 = pd.DataFrame([sub.split(\"\\t\") for sub in other_data])\n",
    "\n",
    "    df2.columns = ['Core','Location','Lat','Lon','Dep','Core Depth','age','d13C']\n",
    "\n",
    "    df2 = df2[['Core','Location','Lat','Lon','Dep','d13C','age']]\n",
    "\n",
    "    df2['d13C'] = [i.rstrip() for i in df2['d13C']]\n",
    "\n",
    "    df2 = df2[df2['age'].astype(float) > float(minn)]\n",
    "    df2 = df2[df2['age'].astype(float) < float(maxx)]\n",
    "\n",
    "    results = pd.concat([df, df2])\n",
    "\n",
    "    results = results[results['d13C'] != '']\n",
    "\n",
    "    results['d13C'] = results['d13C'].astype(float)\n",
    "    results['Lat'] = results['Lat'].astype(float)\n",
    "    results['Lon'] = results['Lon'].astype(float)\n",
    "    results['Dep'] = results['Dep'].astype(float)\n",
    "\n",
    "    return(results)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def pl_cores(minn, maxx):\n",
    "\n",
    "    names = ['Core', 'Location', 'Lat', 'Lon', 'Dep']\n",
    "\n",
    "    # Read in the data\n",
    "    indopac = pd.read_table(\"../Moving_water_mass/Data/Core_files/indopac_core_data_LS16.txt\", delimiter = ',', names = names)\n",
    "    atl = pd.read_table(\"../Moving_water_mass/Data/Core_files/atl_core_data_LS16.txt\", delimiter = ',', names = names)\n",
    "    add = pd.read_table(\"../Moving_water_mass/Data/Core_files/Additional_core_locations.txt\", delimiter = ',', usecols = [0, 1, 2, 3, 4], names = names)\n",
    "\n",
    "    # Join all into a single dataframe\n",
    "    df = indopac.append(atl)\n",
    "    df = df.append(add)\n",
    "    df = df.reset_index(drop = True)\n",
    "\n",
    "    df['d18O names'] = df['Core'] + '_ageLS16.txt'\n",
    "    df['d13C names'] = df['Core'] + '_d13C.txt'\n",
    "\n",
    "    # Loop over the dataset and interpolate each core\n",
    "    i = 0\n",
    "    results_dict = {}\n",
    "\n",
    "    while i < df.count()[0]:\n",
    "\n",
    "        try:\n",
    "            df_d18O = pd.read_table('../Moving_water_mass/Data/Core_files/' + df.loc[i]['d18O names'], delim_whitespace = True, names = ['depth', 'age'], skip_blank_lines = True, na_values = 'NAN')\n",
    "        except:\n",
    "            i += 1\n",
    "            continue\n",
    "        try:\n",
    "            df_d13C = pd.read_table('../Moving_water_mass/Data/Core_files/' + df.loc[i]['d13C names'], delim_whitespace = True, names = ['depth', 'd13C'], skip_blank_lines = True, na_values = 'NAN')\n",
    "        except:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        df_d18O = df_d18O.dropna(subset = ['age']) \n",
    "        df_d13C = df_d13C.dropna(subset = ['d13C'])\n",
    "\n",
    "        df_d18O = df_d18O.reset_index(drop = True)\n",
    "        df_d13C = df_d13C.reset_index(drop = True)\n",
    "\n",
    "        interp = scipy.interpolate.interp1d(df_d18O['depth'], df_d18O['age'], bounds_error = True)\n",
    "        try:\n",
    "            df_d13C['age'] = interp(df_d13C['depth'])\n",
    "        except:\n",
    "            try:\n",
    "                interp2 = scipy.interpolate.interp1d(df_d18O['depth'], df_d18O['age'], bounds_error = False)\n",
    "                df_d13C['age'] = interp2(df_d13C['depth'])\n",
    "            except:\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        df_d13C = df_d13C.dropna(subset = ['age'])\n",
    "        df_d13C = df_d13C.reset_index(drop = True)\n",
    "\n",
    "        df_d13C = df_d13C[df_d13C['age'] > minn]\n",
    "        df_d13C = df_d13C[df_d13C['age'] < maxx]\n",
    "\n",
    "        if len(df_d13C) > 0:\n",
    "            df_results = df.drop(['d18O names', 'd13C names'], axis = 1)\n",
    "            df_results = df_results.loc[df_results.index.repeat(len(df_d13C))].loc[[i]]\n",
    "\n",
    "            df_d13C = df_d13C.drop(['depth'], axis = 1)\n",
    "\n",
    "            df_results = df_results.reset_index(drop = True).join(df_d13C.reset_index(drop = True))\n",
    "            results_dict.update({\n",
    "                df_results.Core[0] : df_results.drop(['Core'], axis = 1)\n",
    "            })\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "    df_results = pd.concat(results_dict).reset_index()\n",
    "    df_results = df_results.rename(columns = {'level_0' : 'Core'})\n",
    "    df_results = df_results.drop(['level_1'], axis = 1)\n",
    "\n",
    "    return(df_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def slicing_data(time_min,time_max,location_filter):\n",
    "    # ################## read in the oliver data using the predefined function\n",
    "\n",
    "    # Age range to look over\n",
    "\n",
    "    df_pl = pl_cores(time_min,time_max)\n",
    "\n",
    "    df_oliver = Oliver_cores(time_min,time_max)\n",
    "    \n",
    "    ###################3 join the dataframes\n",
    "\n",
    "    df_results = df_pl.append(df_oliver)\n",
    "\n",
    "    #################### get only the atlantic cores\n",
    "\n",
    "    df = df_results.reset_index(drop = True)\n",
    "    df['Dep'] = abs(df['Dep'])\n",
    "\n",
    "    df_atl = df[df['Location'] == location_filter]\n",
    "    df_atl = df_atl.reset_index(drop = True)\n",
    "    df_atl['age'] = df_atl.age.astype(float)\n",
    "\n",
    "    # Slice the data\n",
    "    lower = np.arange(time_min, time_max, slice_width)\n",
    "    upper = np.arange(time_min+slice_width, time_max+slice_width, slice_width)\n",
    "\n",
    "    proxy_compilation = {}\n",
    "\n",
    "    for low, up in zip(lower, upper):\n",
    "        df_slice = df_atl[(df_atl['age'] >= low) & (df_atl['age'] < up)]\n",
    "        grouped_slice = df_slice.groupby(['Lat', 'Lon', 'Dep'])\n",
    "\n",
    "        slice_averaged = {}\n",
    "\n",
    "        for key, group in grouped_slice:\n",
    "            group_averaged = group.mean()\n",
    "            group_count = group.count()\n",
    "\n",
    "            group_averaged['count'] = group_count['d13C']\n",
    "            slice_averaged.update({\n",
    "                group.Core.reset_index(drop=True)[0] : pd.DataFrame(group_averaged)\n",
    "            })\n",
    "\n",
    "        slice_averaged = pd.concat(slice_averaged, axis=1).T\n",
    "        slice_averaged = slice_averaged.drop(['age'],axis=1)\n",
    "\n",
    "        proxy_compilation.update({\n",
    "            low : slice_averaged.T\n",
    "        })\n",
    "\n",
    "    proxy_compilation = pd.concat(proxy_compilation,axis=1).T.reset_index(drop=False)\n",
    "    proxy_compilation = proxy_compilation.drop('level_2',axis=1)\n",
    "    proxy_compilation = proxy_compilation.rename(columns={'level_0' : 'lower', 'level_1' : 'Core', 'Dep' : 'Ocean_depth'})\n",
    "    \n",
    "    return(proxy_compilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def slice_interpolation(proxy_compilation,dividing_depth):\n",
    "    # interpolating across the entire dataset\n",
    "\n",
    "    # Add time bounds to the samples table\n",
    "    samples_with_time_period = proxy_compilation\n",
    "\n",
    "    unique_cores = np.unique(samples_with_time_period['Core'])\n",
    "    years_list = np.unique(samples_with_time_period.lower)\n",
    "\n",
    "    samples_with_time_period = samples_with_time_period.reset_index(drop=True)\n",
    "\n",
    "    interpolated_proxies = {}\n",
    "\n",
    "    for unique_core in unique_cores:\n",
    "\n",
    "        # get a single proxy\n",
    "        df_temp = samples_with_time_period[samples_with_time_period['Core'] == unique_core]\n",
    "\n",
    "        try:\n",
    "            # interpolate the dataset\n",
    "            interp = scipy.interpolate.interp1d(df_temp['lower'],\n",
    "                                                df_temp['d13C'],\n",
    "                                                bounds_error = False)\n",
    "            single_proxy_interpolated = pd.DataFrame({'lower' : years_list, 'd13C' : interp(years_list)})        \n",
    "            location_repeated = pd.concat([df_temp.reset_index(0).loc[0,['Lat', 'Lon', 'Ocean_depth','count']]] * len(single_proxy_interpolated), axis=1).T\n",
    "            single_proxy_interpolated = pd.concat([location_repeated.reset_index(drop=True), single_proxy_interpolated.reset_index(drop=True)],axis=1)    \n",
    "\n",
    "            interpolated_proxies.update({\n",
    "                unique_core : single_proxy_interpolated\n",
    "            })        \n",
    "\n",
    "        except ValueError:\n",
    "            interpolated_proxies.update({\n",
    "                unique_core : df_temp.drop('Core',axis=1)\n",
    "            })\n",
    "\n",
    "    interpolated_samples = pd.concat(interpolated_proxies).reset_index(drop=False).rename(columns={'level_0':'Core'}).drop(['level_1'],axis=1)\n",
    "    interpolated_samples_backup = interpolated_samples.copy()\n",
    "\n",
    "    # Drop count because this count doesn't make sense\n",
    "    interpolated_samples = interpolated_samples.drop('count',axis=1)\n",
    "\n",
    "    # Merge the original dataframe to get counts back (nan for interpolated samples)\n",
    "    interpolated_samples = pd.merge(interpolated_samples, proxy_compilation[['lower','Core','count']],\n",
    "                                    how='outer', left_on=['lower','Core'], right_on=['lower','Core'])\n",
    "\n",
    "    # Drop all nan d13C values (interpolation tried but out of range)\n",
    "    interpolated_samples = interpolated_samples[np.isfinite(interpolated_samples['d13C'])]\n",
    "\n",
    "    # Divide into a shallow and deep dataframe\n",
    "    shallower_samples = interpolated_samples[interpolated_samples['Ocean_depth'] < dividing_depth]\n",
    "    deeper_samples = interpolated_samples[interpolated_samples['Ocean_depth'] > dividing_depth]\n",
    "    \n",
    "    return(interpolated_samples, shallower_samples, deeper_samples)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_latex_table(proxy_compilation,latex_name):\n",
    "    # Save list of cores to latex table to include in paper\n",
    "    latex_table = proxy_compilation.drop(['d13C','count','lower'],axis=1)\n",
    "    latex_table['Reference'] = '-'\n",
    "    latex_table = latex_table.drop_duplicates()\n",
    "\n",
    "    # Rename columns\n",
    "    latex_table = latex_table.rename(columns={'Lat':'Latitude','Lon' : 'Longitude', 'Ocean_depth' : 'Depth (m)'})\n",
    "    latex_table['Latitude'] = [str(round(x, 2)) for x in latex_table.Latitude]\n",
    "    latex_table['Longitude'] = [str(round(x, 2)) for x in latex_table.Longitude]\n",
    "    latex_table.sort_values(by='Core',inplace=True)\n",
    "\n",
    "    # Convert to string of latex markdown\n",
    "    latex_string = latex_table.to_latex(index=False,longtable=True)\n",
    "\n",
    "    # Reformat some parts of the latex table\n",
    "    latex_string = latex_string.replace('\\\\toprule','')\n",
    "    latex_string = latex_string.replace('\\\\midrule','')\n",
    "    latex_string = latex_string.replace('\\\\bottomrule','')\n",
    "\n",
    "    # Write to a file\n",
    "    file1 = open(latex_name,\"w\") \n",
    "    file1.write(latex_string) \n",
    "    file1.close() #to change file access modes \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Atlantic_regions(interpolated_samples):\n",
    "    # Get volume weight average with time slices\n",
    "    interpolated_samples['weights'] = np.nan\n",
    "    interpolated_samples['regions'] = np.nan    \n",
    "\n",
    "    # NEA\n",
    "    interpolated_samples.loc[((interpolated_samples['Lon'] < 20) | (interpolated_samples['Lon'] > (-33))) & (interpolated_samples['Lat'] > 0.1),\n",
    "                            'weights'] = 4.3\n",
    "    interpolated_samples.loc[((interpolated_samples['Lon'] < 20) | (interpolated_samples['Lon'] > (-33))) & (interpolated_samples['Lat'] > 0.1),\n",
    "                            'regions'] = 'NEA'\n",
    "    \n",
    "\n",
    "    # NWA\n",
    "    interpolated_samples.loc[(interpolated_samples['Lon'] < (-33)) & (interpolated_samples['Lon'] > (-180)) & (interpolated_samples['Lat'] > 0.1),\n",
    "                             'weights'] = 4.9\n",
    "    interpolated_samples.loc[(interpolated_samples['Lon'] < (-33)) & (interpolated_samples['Lon'] > (-180)) & (interpolated_samples['Lat'] > 0.1),\n",
    "                             'regions'] = 'NWA'\n",
    "\n",
    "    # SEA\n",
    "    interpolated_samples.loc[((interpolated_samples['Lon'] < 30) | (interpolated_samples['Lon'] > (-14.6))) & (interpolated_samples['Lat'] < 0) & (interpolated_samples['Lat'] > -55),\n",
    "                             'weights'] = 3.5\n",
    "    interpolated_samples.loc[((interpolated_samples['Lon'] < 30) | (interpolated_samples['Lon'] > (-14.6))) & (interpolated_samples['Lat'] < 0) & (interpolated_samples['Lat'] > -55),\n",
    "                             'regions'] = 'SEA'\n",
    "    \n",
    "    # SA\n",
    "    interpolated_samples.loc[((interpolated_samples['Lon'] < 30) | (interpolated_samples['Lon'] > (-22))) & (interpolated_samples['Lat'] < -40) & (interpolated_samples['Lat'] > -55),\n",
    "                            'weights'] = 0.7\n",
    "    interpolated_samples.loc[((interpolated_samples['Lon'] < 30) | (interpolated_samples['Lon'] > (-22))) & (interpolated_samples['Lat'] < -40) & (interpolated_samples['Lat'] > -55),\n",
    "                            'regions'] = 'SA'    \n",
    "\n",
    "    # SWA\n",
    "    interpolated_samples.loc[(interpolated_samples['Lon'] > (-60)) & (interpolated_samples['Lon'] < (-14.6)) & (interpolated_samples['Lat'] < 0) & (interpolated_samples['Lat'] > -55),\n",
    "                            'weights'] = 5.0\n",
    "    interpolated_samples.loc[(interpolated_samples['Lon'] > (-60)) & (interpolated_samples['Lon'] < (-14.6)) & (interpolated_samples['Lat'] < 0) & (interpolated_samples['Lat'] > -55),\n",
    "                            'regions'] = 'SWA'    \n",
    "    \n",
    "    return(interpolated_samples)\n",
    "    \n",
    "def Pacific_regions(interpolated_samples):\n",
    "    # Get volume weight average with time slices\n",
    "    interpolated_samples['weights'] = np.nan\n",
    "    interpolated_samples['regions'] = np.nan      \n",
    "\n",
    "    # North\n",
    "    interpolated_samples.loc[(interpolated_samples['Lat'] > 0) ,'weights'] = 21.2\n",
    "    interpolated_samples.loc[(interpolated_samples['Lat'] > 0) ,'regions'] = 'NP'    \n",
    "\n",
    "    # South\n",
    "    interpolated_samples.loc[(interpolated_samples['Lat'] < 0) ,'weights'] = 23.9\n",
    "    interpolated_samples.loc[(interpolated_samples['Lat'] < 0) ,'regions'] = 'SP'\n",
    "    \n",
    "    return(interpolated_samples)\n",
    "\n",
    "def Indian_regions(interpolated_samples):\n",
    "    # Get volume weight average with time slices # Use a single region for the Indian Ocean\n",
    "    interpolated_samples['weights'] = 1\n",
    "    interpolated_samples['regions'] = 'I'\n",
    "    \n",
    "    return(interpolated_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averaging_by_region(interpolated_samples):\n",
    "    # group the cores based on the age and the region (indicated by weights)\n",
    "    grouped_by_age_region = interpolated_samples.groupby(['lower', 'regions'])\n",
    "    \n",
    "    averaged_by_age_region = {}\n",
    "    stdev_by_age_region = {}\n",
    "    measurement_count = {}\n",
    "    core_count = {}\n",
    "    CI_by_age_region = {}\n",
    "    \n",
    "    for key, group in grouped_by_age_region:\n",
    "        \n",
    "        # find group means\n",
    "        averaged_by_age_region.update({\n",
    "            key: np.mean(group)\n",
    "        })\n",
    "        # find total number of cores\n",
    "        core_count.update({\n",
    "            key: len(group['count'].dropna())\n",
    "        }) \n",
    "        # find total measurement count\n",
    "        measurement_count.update({\n",
    "            key: np.nansum(group['count'])\n",
    "        })\n",
    "        # find the standard deviation across the slices and cores\n",
    "        stdev_by_age_region.update({\n",
    "            key: np.std(group['d13C'])\n",
    "        })\n",
    "        # express variation as a confidence interval\n",
    "        CI_by_age_region.update({\n",
    "            key: 1.96 * np.std(group['d13C'])/(np.sqrt(len(group['count'].dropna()))) #CI of 95%, 1.96 is zscore\n",
    "        })        \n",
    "        \n",
    "    # Convert dictionaries to dataframes\n",
    "    averaged_by_age_region = pd.concat(averaged_by_age_region,axis=1).T.reset_index(drop=False).rename({'level_1':'regions'},axis=1).drop('level_0',axis=1)\n",
    "    measurement_count = pd.DataFrame.from_dict(measurement_count,orient='index')\n",
    "    core_count = pd.DataFrame.from_dict(core_count,orient='index')\n",
    "    stdev_by_age_region = pd.DataFrame.from_dict(stdev_by_age_region,orient='index')\n",
    "    CI_by_age_region = pd.DataFrame.from_dict(CI_by_age_region,orient='index')    \n",
    "    \n",
    "    # Add columns of number of cores and number of measurements to the dataframe\n",
    "    averaged_by_age_region['measurement_count'] = list(measurement_count[0])\n",
    "    averaged_by_age_region['core_count'] = list(core_count[0])  \n",
    "    averaged_by_age_region['slice_stdev'] = list(stdev_by_age_region[0])\n",
    "    averaged_by_age_region['CI'] = list(CI_by_age_region[0])\n",
    "    \n",
    "    # Now there is one values for each region (weight) and each year combination\n",
    "    # Group by years and use weights to find the average d13C for that time period\n",
    "\n",
    "    grouped_by_age = averaged_by_age_region.drop('regions',axis=1).groupby('lower')\n",
    "\n",
    "    averaged_by_age = {}\n",
    "    stdev_by_age = {}\n",
    "\n",
    "    # find group means\n",
    "    for key, group in grouped_by_age:\n",
    "\n",
    "        # Find the normal average\n",
    "        avg = np.mean(group.d13C)\n",
    "\n",
    "        averaged_by_age.update({\n",
    "            np.mean(group['lower']) : np.sum(group['d13C'] * group['weights'])/np.sum(group['weights'])\n",
    "        })\n",
    "        stdev_by_age.update({\n",
    "            np.mean(group['lower']) : np.average((group.d13C-avg)**2,weights=group.weights)\n",
    "        }) \n",
    "\n",
    "\n",
    "    averaged_by_age = pd.DataFrame.from_dict(averaged_by_age,orient='index').reset_index(drop=False).rename(columns={'index' : 'lower', 0 : 'd13C'})\n",
    "    stdev_by_age = pd.DataFrame.from_dict(stdev_by_age,orient='index').reset_index(drop=False).rename(columns={'index' : 'lower', 0 : 'stdev'})\n",
    "\n",
    "    averaged_by_age = pd.merge(left=averaged_by_age,right=stdev_by_age)\n",
    "    averaged_by_age = averaged_by_age.sort_values('lower')\n",
    "    \n",
    "    return(averaged_by_age,averaged_by_age_region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sort_data(time_min,time_max,region_func,interp_data_name,notinterp_data_name,latex_name,location_filter):\n",
    "    # Slice the data\n",
    "    proxy_compilation = slicing_data(time_min,time_max,location_filter)\n",
    "    \n",
    "    # Interpolate the data and divide into a shallow and deep dataframe\n",
    "    interpolated_samples, shallower_samples, deeper_samples = slice_interpolation(\n",
    "        proxy_compilation,dividing_depth)\n",
    "    \n",
    "    # write latex table for paper\n",
    "    make_latex_table(proxy_compilation,latex_name)\n",
    "    \n",
    "    # get region weights\n",
    "    interpolated_samples = region_func(interpolated_samples)\n",
    "\n",
    "    # Get regional average and ocean basin averages\n",
    "    averaged_by_age, averaged_by_age_region = averaging_by_region(interpolated_samples)\n",
    "\n",
    "    # drop weights (because I'm not using them now)\n",
    "    interpolated_samples = interpolated_samples.drop('weights',axis=1)\n",
    "\n",
    "    # Save the datasets\n",
    "    averaged_by_age.to_csv(notinterp_data_name, index=False)\n",
    "    interpolated_samples.to_csv(interp_data_name,index=False)\n",
    "    \n",
    "    return(averaged_by_age, averaged_by_age_region, interpolated_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set time periods\n",
    "Hol_min = 2\n",
    "Hol_max = 8\n",
    "LIG_min = 118\n",
    "LIG_max = 130\n",
    "\n",
    "# set up other variables\n",
    "\n",
    "# Holocene, Atlantic\n",
    "Hol_atl_latex_name = \"Figures/Holocene_cores_tables_Atlantic.tex\"\n",
    "Hol_atl_nointerp_data_name = \"Data/Atlantic_Holocene_profile_PL.csv\"\n",
    "Hol_atl_interp_data_name = \"/srv/ccrc/data06/z5145948/Moving_water_mass/Data/interpolated_Hol_Atlantic.csv\"\n",
    "\n",
    "# LIG, Atlantic\n",
    "LIG_atl_latex_name = \"Figures/LIG_cores_tables_Atlantic.tex\"\n",
    "LIG_atl_nointerp_data_name = \"Data/Atlantic_LIG_profile_PL.csv\"\n",
    "LIG_atl_interp_data_name = \"/srv/ccrc/data06/z5145948/Moving_water_mass/Data/interpolated_LIG_Atlantic.csv\"\n",
    "\n",
    "# Holocene, Pacific\n",
    "Hol_pac_latex_name = \"Figures/Holocene_cores_tables_Pacific.tex\"\n",
    "Hol_pac_nointerp_data_name = \"Data/Pacific_Holocene_profile_PL.csv\"\n",
    "Hol_pac_interp_data_name = \"/srv/ccrc/data06/z5145948/Moving_water_mass/Data/interpolated_Hol_Pacific.csv\"\n",
    "\n",
    "# LIG, Pacific\n",
    "LIG_pac_latex_name = \"Figures/LIG_cores_tables_Pacific.tex\"\n",
    "LIG_pac_nointerp_data_name = \"Data/Pacific_LIG_profile_PL.csv\"\n",
    "LIG_pac_interp_data_name = \"/srv/ccrc/data06/z5145948/Moving_water_mass/Data/interpolated_LIG_Pacific.csv\"\n",
    "\n",
    "# Holocene, Indian\n",
    "Hol_ind_latex_name = \"Figures/Holocene_cores_tables_Indian.tex\"\n",
    "Hol_ind_nointerp_data_name = \"Data/Indian_Holocene_profile_PL.csv\"\n",
    "Hol_ind_interp_data_name = \"/srv/ccrc/data06/z5145948/Moving_water_mass/Data/interpolated_Hol_Indian.csv\"\n",
    "\n",
    "# LIG, Indian\n",
    "LIG_ind_latex_name = \"Figures/LIG_cores_tables_Indian.tex\"\n",
    "LIG_ind_nointerp_data_name = \"Data/Indian_LIG_profile_PL.csv\"\n",
    "LIG_ind_interp_data_name = \"/srv/ccrc/data06/z5145948/Moving_water_mass/Data/interpolated_LIG_Indian.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/ccrc/data06/z5145948/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:37: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "/srv/ccrc/data06/z5145948/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# Run function for each scenario (three oceans, two time periods)\n",
    "\n",
    "interpolated_results_dict = {}\n",
    "average_age_interpolated_results_dict = {}\n",
    "average_age_region_interpolated_results_dict = {}\n",
    "\n",
    "for label,time_min,time_max,region_func,interp_data_name,notinterp_data_name,latex_name,location_filter in zip(\n",
    "    ['Hol_atl','LIG_atl','Hol_pac','LIG_pac','Hol_ind','LIG_ind'],\n",
    "    [Hol_min,LIG_min,Hol_min,LIG_min,Hol_min,LIG_min],\n",
    "    [Hol_max,LIG_max,Hol_max,LIG_max,Hol_max,LIG_max],\n",
    "    [Atlantic_regions,Atlantic_regions,Pacific_regions,Pacific_regions,Indian_regions,Indian_regions],\n",
    "    [Hol_atl_interp_data_name,LIG_atl_interp_data_name,Hol_pac_interp_data_name,LIG_pac_interp_data_name,Hol_ind_interp_data_name,LIG_ind_interp_data_name],\n",
    "    [Hol_atl_nointerp_data_name,LIG_atl_nointerp_data_name,Hol_pac_nointerp_data_name,LIG_pac_nointerp_data_name,Hol_ind_nointerp_data_name,LIG_ind_nointerp_data_name],\n",
    "    [Hol_atl_latex_name,LIG_atl_latex_name,Hol_pac_latex_name,LIG_pac_latex_name,Hol_ind_latex_name,LIG_ind_latex_name],\n",
    "    ['Atlantic','Atlantic','Pacific','Pacific','Indian','Indian']):\n",
    "    \n",
    "    # Run function\n",
    "    averaged_by_age, averaged_by_age_region, interpolated_samples = sort_data(\n",
    "        time_min,time_max,region_func,interp_data_name,notinterp_data_name,latex_name,location_filter\n",
    "    )\n",
    "    \n",
    "    # Add results to dictionary\n",
    "    interpolated_results_dict.update({\n",
    "        label : interpolated_samples\n",
    "    })\n",
    "    average_age_interpolated_results_dict.update({\n",
    "        label : averaged_by_age\n",
    "    })   \n",
    "    average_age_region_interpolated_results_dict.update({\n",
    "        label : averaged_by_age_region\n",
    "    })     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/ccrc/data06/z5145948/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n",
      "/srv/ccrc/data06/z5145948/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Create results dataframes from dictionaries\n",
    "df_interpolated_results = pd.concat(interpolated_results_dict).reset_index(drop=True)\n",
    "df_average_age_interpolated_results = pd.concat(average_age_interpolated_results_dict).reset_index().drop('level_1',axis=1).rename(columns=\n",
    "                                                                                 {'level_0':'location'})\n",
    "df_average_age_region_interpolated_results = pd.concat(average_age_region_interpolated_results_dict).reset_index().drop('level_1',axis=1).rename(columns=\n",
    "                                                                                 {'level_0':'location'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# peak LIG and peak Holocene time periods\n",
    "LIG_peak_min = 120\n",
    "LIG_peak_max = 125\n",
    "Hol_peak_min = 4\n",
    "Hol_peak_max = 7\n",
    "\n",
    "# find average results of peak period\n",
    "df_LIG_peak = df_average_age_region_interpolated_results[(df_average_age_region_interpolated_results['lower'] > LIG_peak_min) & (df_average_age_region_interpolated_results['lower'] < LIG_peak_max)]\n",
    "df_Hol_peak = df_average_age_region_interpolated_results[(df_average_age_region_interpolated_results['lower'] > Hol_peak_min) & (df_average_age_region_interpolated_results['lower'] < Hol_peak_max)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peak Holocene and LIG summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Average over all time slices for LIG \n",
    "LIG_peak_averaged_dict = {}\n",
    "for index,group in df_LIG_peak.groupby(['location','regions']):\n",
    "    LIG_peak_averaged_dict.update({\n",
    "        index:pd.DataFrame(np.mean(group)).T\n",
    "    })\n",
    "df_LIG_peak_averaged = pd.concat(LIG_peak_averaged_dict)\n",
    "    \n",
    "# Average over all time slices for Holocene\n",
    "Hol_peak_averaged_dict = {}\n",
    "for index,group in df_Hol_peak.groupby(['location','regions']):\n",
    "    Hol_peak_averaged_dict.update({\n",
    "        index:pd.DataFrame(np.mean(group)).T\n",
    "    })    \n",
    "df_Hol_peak_averaged = pd.concat(Hol_peak_averaged_dict)    \n",
    "    \n",
    "# Format dataframes for merge\n",
    "df_Hol_peak_averaged = df_Hol_peak_averaged.reset_index().drop(['weights','lower','count','level_2','level_0'],axis=1)\n",
    "df_Hol_peak_averaged = df_Hol_peak_averaged.rename(columns={'level_1':'Location'})\n",
    "df_Hol_peak_averaged.set_index(['Location'],inplace=True)\n",
    "\n",
    "df_LIG_peak_averaged = df_LIG_peak_averaged.reset_index().drop(['lower','count','level_2','level_0'],axis=1)\n",
    "df_LIG_peak_averaged = df_LIG_peak_averaged.rename(columns={'level_1':'Location'})\n",
    "df_LIG_peak_averaged.set_index(['Location'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearange order of columns\n",
    "df_LIG_peak_averaged_sorted = df_LIG_peak_averaged[['d13C','measurement_count','core_count','slice_stdev','CI']]\n",
    "df_Hol_peak_averaged_sorted = df_Hol_peak_averaged[['d13C','measurement_count','core_count','slice_stdev','CI']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join results into a single table\n",
    "combined_results = df_Hol_peak_averaged_sorted.join(df_LIG_peak_averaged_sorted,lsuffix=' Holocene',rsuffix=' LIG',sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d13C Holocene</th>\n",
       "      <th>measurement_count Holocene</th>\n",
       "      <th>core_count Holocene</th>\n",
       "      <th>slice_stdev Holocene</th>\n",
       "      <th>CI Holocene</th>\n",
       "      <th>d13C LIG</th>\n",
       "      <th>measurement_count LIG</th>\n",
       "      <th>core_count LIG</th>\n",
       "      <th>slice_stdev LIG</th>\n",
       "      <th>CI LIG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NEA</th>\n",
       "      <td>0.938611</td>\n",
       "      <td>72.5</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0.198680</td>\n",
       "      <td>0.062757</td>\n",
       "      <td>0.761522</td>\n",
       "      <td>26.00</td>\n",
       "      <td>11.75</td>\n",
       "      <td>0.209845</td>\n",
       "      <td>0.123354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NWA</th>\n",
       "      <td>0.806101</td>\n",
       "      <td>28.5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.238433</td>\n",
       "      <td>0.125678</td>\n",
       "      <td>0.658415</td>\n",
       "      <td>38.50</td>\n",
       "      <td>7.25</td>\n",
       "      <td>0.230390</td>\n",
       "      <td>0.173702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SA</th>\n",
       "      <td>-0.025208</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.175940</td>\n",
       "      <td>0.198458</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.258489</td>\n",
       "      <td>0.429321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEA</th>\n",
       "      <td>0.629537</td>\n",
       "      <td>10.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.254233</td>\n",
       "      <td>0.161938</td>\n",
       "      <td>0.538570</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.226530</td>\n",
       "      <td>0.182970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA</th>\n",
       "      <td>0.916708</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.422232</td>\n",
       "      <td>0.406669</td>\n",
       "      <td>0.476493</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.205076</td>\n",
       "      <td>0.337902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0.173000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.238893</td>\n",
       "      <td>0.223024</td>\n",
       "      <td>0.036607</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.206315</td>\n",
       "      <td>0.244906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NP</th>\n",
       "      <td>0.051040</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.215794</td>\n",
       "      <td>0.159862</td>\n",
       "      <td>-0.091236</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.267256</td>\n",
       "      <td>0.286405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP</th>\n",
       "      <td>0.474761</td>\n",
       "      <td>12.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.263107</td>\n",
       "      <td>0.257845</td>\n",
       "      <td>0.136365</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.196790</td>\n",
       "      <td>0.332165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          d13C Holocene  measurement_count Holocene  core_count Holocene  \\\n",
       "Location                                                                   \n",
       "NEA            0.938611                        72.5                 38.5   \n",
       "NWA            0.806101                        28.5                 14.0   \n",
       "SA            -0.025208                         6.5                  3.0   \n",
       "SEA            0.629537                        10.5                  9.5   \n",
       "SWA            0.916708                         6.0                  4.5   \n",
       "I              0.173000                         6.0                  4.5   \n",
       "NP             0.051040                        12.0                  7.0   \n",
       "SP             0.474761                        12.5                  4.0   \n",
       "\n",
       "          slice_stdev Holocene  CI Holocene  d13C LIG  measurement_count LIG  \\\n",
       "Location                                                                       \n",
       "NEA                   0.198680     0.062757  0.761522                  26.00   \n",
       "NWA                   0.238433     0.125678  0.658415                  38.50   \n",
       "SA                    0.175940     0.198458  0.011667                   2.25   \n",
       "SEA                   0.254233     0.161938  0.538570                   6.00   \n",
       "SWA                   0.422232     0.406669  0.476493                   1.50   \n",
       "I                     0.238893     0.223024  0.036607                   3.00   \n",
       "NP                    0.215794     0.159862 -0.091236                   5.00   \n",
       "SP                    0.263107     0.257845  0.136365                   1.50   \n",
       "\n",
       "          core_count LIG  slice_stdev LIG    CI LIG  \n",
       "Location                                             \n",
       "NEA                11.75         0.209845  0.123354  \n",
       "NWA                 7.25         0.230390  0.173702  \n",
       "SA                  1.50         0.258489  0.429321  \n",
       "SEA                 6.00         0.226530  0.182970  \n",
       "SWA                 1.50         0.205076  0.337902  \n",
       "I                   2.75         0.206315  0.244906  \n",
       "NP                  3.50         0.267256  0.286405  \n",
       "SP                  1.50         0.196790  0.332165  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the numbers\n",
    "combined_results[['core_count Holocene','measurement_count Holocene',\n",
    "                  'core_count LIG','measurement_count LIG']] = \\\n",
    "                    combined_results[['core_count Holocene','measurement_count Holocene',\n",
    "                    'core_count LIG','measurement_count LIG']].astype(int)\n",
    "combined_results = combined_results.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d13C Holocene</th>\n",
       "      <th>measurement_count Holocene</th>\n",
       "      <th>core_count Holocene</th>\n",
       "      <th>slice_stdev Holocene</th>\n",
       "      <th>CI Holocene</th>\n",
       "      <th>d13C LIG</th>\n",
       "      <th>measurement_count LIG</th>\n",
       "      <th>core_count LIG</th>\n",
       "      <th>slice_stdev LIG</th>\n",
       "      <th>CI LIG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NEA</th>\n",
       "      <td>0.94</td>\n",
       "      <td>72</td>\n",
       "      <td>38</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.76</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NWA</th>\n",
       "      <td>0.81</td>\n",
       "      <td>28</td>\n",
       "      <td>14</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.66</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SA</th>\n",
       "      <td>-0.03</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEA</th>\n",
       "      <td>0.63</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.54</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA</th>\n",
       "      <td>0.92</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0.17</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NP</th>\n",
       "      <td>0.05</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP</th>\n",
       "      <td>0.47</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          d13C Holocene  measurement_count Holocene  core_count Holocene  \\\n",
       "Location                                                                   \n",
       "NEA                0.94                          72                   38   \n",
       "NWA                0.81                          28                   14   \n",
       "SA                -0.03                           6                    3   \n",
       "SEA                0.63                          10                    9   \n",
       "SWA                0.92                           6                    4   \n",
       "I                  0.17                           6                    4   \n",
       "NP                 0.05                          12                    7   \n",
       "SP                 0.47                          12                    4   \n",
       "\n",
       "          slice_stdev Holocene  CI Holocene  d13C LIG  measurement_count LIG  \\\n",
       "Location                                                                       \n",
       "NEA                       0.20         0.06      0.76                     26   \n",
       "NWA                       0.24         0.13      0.66                     38   \n",
       "SA                        0.18         0.20      0.01                      2   \n",
       "SEA                       0.25         0.16      0.54                      6   \n",
       "SWA                       0.42         0.41      0.48                      1   \n",
       "I                         0.24         0.22      0.04                      3   \n",
       "NP                        0.22         0.16     -0.09                      5   \n",
       "SP                        0.26         0.26      0.14                      1   \n",
       "\n",
       "          core_count LIG  slice_stdev LIG  CI LIG  \n",
       "Location                                           \n",
       "NEA                   11             0.21    0.12  \n",
       "NWA                    7             0.23    0.17  \n",
       "SA                     1             0.26    0.43  \n",
       "SEA                    6             0.23    0.18  \n",
       "SWA                    1             0.21    0.34  \n",
       "I                      2             0.21    0.24  \n",
       "NP                     3             0.27    0.29  \n",
       "SP                     1             0.20    0.33  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to multilevel index\n",
    "df_latex = combined_results\n",
    "df_latex.reset_index(drop=False,inplace=True)\n",
    "df_latex.columns = pd.MultiIndex.from_arrays([['Text',\n",
    "                             'Holocene','Holocene','Holocene','Holocene','Holocene',\n",
    "                             'LIG','LIG','LIG','LIG','LIG',],\n",
    "                            ['Location','d13C','measurement_count','core_count','slice_stdev','CI','d13C','measurement_count','core_count','slice_stdev','CI']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{longtable}{lcccccccccc}\\hline\n",
      "\n",
      "    \\multicolumn{1}{l}{} & \\multicolumn{5}{|c|}{Holocene} & \\multicolumn{5}{|c|}{LIG} \\\\ \\hline\n",
      "Location &     \\thead{$\\delta^{13}$C \\\\ (\\permil)} & \\thead{Number of \\\\ measurements} & \\thead{Number of \\\\ cores} & \\thead{$\\sigma$ \\\\ (\\permil)} &    \\thead{Confidence Interval \\\\ (\\permil)} &  \\thead{$\\delta^{13}$C \\\\ (\\permil)} & \\thead{Number of \\\\ measurements} & \\thead{Number of \\\\ cores} & \\thead{$\\sigma$ \\\\ (\\permil)} &    \\thead{Confidence Interval \\\\ (\\permil)} \\\\ \\hline\n",
      "\n",
      "\\endhead\n",
      "\n",
      "\\multicolumn{11}{r}{{Continued on next page}} \\\\ \\hline\n",
      "\n",
      "\\endfoot\n",
      "\n",
      "\n",
      "\\endlastfoot\n",
      "     NEA &     0.94 &                72 &         38 &        0.20 &  0.06 &  0.76 &                26 &         11 &        0.21 &  0.12 \\\\ \\hline\n",
      "     NWA &     0.81 &                28 &         14 &        0.24 &  0.13 &  0.66 &                38 &          7 &        0.23 &  0.17 \\\\ \\hline\n",
      "      SA &    -0.03 &                 6 &          3 &        0.18 &  0.20 &  0.01 &                 2 &          1 &        0.26 &  0.43 \\\\ \\hline\n",
      "     SEA &     0.63 &                10 &          9 &        0.25 &  0.16 &  0.54 &                 6 &          6 &        0.23 &  0.18 \\\\ \\hline\n",
      "     SWA &     0.92 &                 6 &          4 &        0.42 &  0.41 &  0.48 &                 1 &          1 &        0.21 &  0.34 \\\\ \\hline\n",
      "       I &     0.17 &                 6 &          4 &        0.24 &  0.22 &  0.04 &                 3 &          2 &        0.21 &  0.24 \\\\ \\hline\n",
      "      NP &     0.05 &                12 &          7 &        0.22 &  0.16 & -0.09 &                 5 &          3 &        0.27 &  0.29 \\\\ \\hline\n",
      "      SP &     0.47 &                12 &          4 &        0.26 &  0.26 &  0.14 &                 1 &          1 &        0.20 &  0.33 \\\\ \\hline\n",
      "\\end{longtable}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to string of latex markdown\n",
    "latex_string = df_latex.to_latex(index=False,longtable=True)\n",
    "\n",
    "# Reformat some parts of the latex table\n",
    "latex_string = latex_string.replace('\\\\toprule','')\n",
    "latex_string = latex_string.replace('\\\\midrule','')\n",
    "latex_string = latex_string.replace('\\\\bottomrule','')\n",
    "latex_string = latex_string.replace('\\\\\\\\','\\\\\\\\ \\hline')\n",
    "# Change column names\n",
    "latex_string = latex_string.replace('measurement\\_count','\\\\thead{Number of \\\\\\\\ measurements}')\n",
    "latex_string = latex_string.replace('core\\_count','\\\\thead{Number of \\\\\\\\ cores}')\n",
    "latex_string = latex_string.replace('slice\\_stdev','\\\\thead{$\\sigma$ \\\\\\\\ (\\permil)}')\n",
    "latex_string = latex_string.replace('CI','\\\\thead{Confidence Interval \\\\\\\\ (\\permil)}')\n",
    "latex_string = latex_string.replace('d13C','\\\\thead{$\\delta^{13}$C \\\\\\\\ (\\permil)}')\n",
    "latex_string = latex_string.replace('Text','\\\\multicolumn{1}{l}{}')\n",
    "latex_string = latex_string.replace('{5}{l}','{5}{|c|}')\n",
    "\n",
    "# Add caption to latex table\n",
    "caption = [\n",
    "    '\\caption{Region breakdown of $\\delta^{13}$C data during the Holocene and LIG.',\n",
    "    'Number of measures and number of cores in each region is provided,',\n",
    "    'with $\\sigma$ representing one standard deviation between time slices,',\n",
    "    'and the associated 95\\\\% confidence interval (CI).}',\n",
    "    '\\label{regional_summary_tb}'\n",
    "]\n",
    "\n",
    "caption = ' '.join(caption)\n",
    "\n",
    "latex_string = latex_string.replace('{lrrrrrrrrrr}','{lcccccccccc}\\\\hline')\n",
    "\n",
    "\n",
    "print(latex_string)\n",
    "\n",
    "latex_name = 'Figures/regional_summary_table_PL.tex'\n",
    "# Write to a file\n",
    "file1 = open(latex_name,\"w\") \n",
    "file1.write(latex_string) \n",
    "file1.close() #to change file access modes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
