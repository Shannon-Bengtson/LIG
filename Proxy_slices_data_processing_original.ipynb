{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z5145948/anaconda3/lib/python3.6/site-packages/pandas/compat/_optional.py:106: UserWarning: Pandas requires version '1.2.1' or newer of 'bottleneck' (version '1.2.0' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/z5145948/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1401: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from netCDF4 import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable  \n",
    "import matplotlib\n",
    "import scipy\n",
    "import scipy.interpolate\n",
    "import sys\n",
    "sys.path.insert(0, '/srv/ccrc/data06/z5145948/Python/python_from_R/Holocene/sampled_models/plotting_files/')\n",
    "from plott import plott\n",
    "import scipy.interpolate\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "matplotlib.use('agg')\n",
    "from Cross_section import Cross_section\n",
    "from Proxy_graph_masked import Proxy_graph\n",
    "import Config\n",
    "from Map_plot import Map_plot\n",
    "from collections import Counter\n",
    "import scipy.stats as stats\n",
    "from pylab import *\n",
    "rcParams['legend.numpoints'] = 1\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import time\n",
    "from IPython import embed\n",
    "import os\n",
    "import ast\n",
    "from matplotlib import gridspec\n",
    "\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "matplotlib.rcParams['text.latex.preamble'] = [\n",
    "    r'\\usepackage{wasysym}',\n",
    "    r'\\usepackage{textcomp}']\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 1200\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import standardised latitudes and depths\n",
    "lat_standardised = np.genfromtxt('/srv/ccrc/data06/z5145948/Moving_water_mass/Data/latitude_levels.csv', delimiter=',')\n",
    "dep_standardised = np.genfromtxt('/srv/ccrc/data06/z5145948/Moving_water_mass/Data/depth_levels.csv', delimiter=',')\n",
    "\n",
    "# File saving location\n",
    "overleaf_loc = '/home/z5145948/Dropbox/Apps/Overleaf/Paper_2/Figures'\n",
    "\n",
    "# Set universal dividing depth\n",
    "dividing_depth = 2500\n",
    "\n",
    "# Set colors\n",
    "alpha = 0.5\n",
    "point_color_deep = (0,0,1,alpha)#'blue'\n",
    "point_color_shallow = (1,0,0,alpha)#'red'\n",
    "line_color_deep = 'cyan'\n",
    "line_color_shallow = 'magenta'\n",
    "\n",
    "# Font size\n",
    "fontsize = 20\n",
    "\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : fontsize}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "# Separate into different areas\n",
    "point_size_power = 1\n",
    "points_size_multiplier = 10\n",
    "south_lat_max = -15\n",
    "north_lat_min = 15\n",
    "lw = 4\n",
    "edgewidth=0.5\n",
    "size = 40\n",
    "weighting = False\n",
    "\n",
    "slice_width = 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####3 reading data function\n",
    "\n",
    "def read_data(folder):\n",
    "\n",
    "    # Import simulation details (summary) as dataframe\n",
    "    summary = pd.read_csv(folder + '_summary.txt', delimiter = ' ')\n",
    "\n",
    "    # Import simulation outputs\n",
    "    fh = Dataset(folder + 'output.nc')\n",
    "    proxy_simulations = fh.variables['var1_1'][:] \n",
    "\n",
    "    # Import samples (proxy data)\n",
    "    samples = pd.read_csv( folder + '_samples.txt', delimiter = ' ')\n",
    "\n",
    "    # Drop all unnecessary rows in summary\n",
    "    summary = summary.drop(['type', 'Row.names', 'reps', 'model', 'dataset', 'filename', 'success.rate'], axis = 1)\n",
    "\n",
    "    # Convert run.no to netcdf file index\n",
    "    summary['run.no'] = summary['run.no'] - 1\n",
    "    samples['run.no'] = samples['run.no'] - 1\n",
    "    \n",
    "    return(proxy_simulations, samples, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function for reading and including cores that are in the Oliver compilation but not in the Peterson data set\n",
    "\n",
    "def Oliver_cores(minn, maxx):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    folder_location = '/srv/ccrc/data06/z5145948/Moving_water_mass/Data/Core_files/'\n",
    "    file_mat = ['GeoB4403_2.txt',\n",
    "             'GeoB1028_5.txt',\n",
    "             'GeoB2109_1.txt',\n",
    "             'GeoB3801_6.txt',\n",
    "             'V22_38.txt',\n",
    "             'V28_56.txt',\n",
    "             'V27_20.txt',\n",
    "             'RC12_339.txt',\n",
    "             'V32_128.txt',\n",
    "             'GIK16772_1.txt',\n",
    "             'MD96_2080.txt',\n",
    "             'MD06_3018.txt',\n",
    "             'NEAP18K.txt',\n",
    "             'KNR140_37JPC.txt']\n",
    "\n",
    "\n",
    "    location = ['Atlantic,','Atlantic,','Atlantic,','Atlantic,','Atlantic,','Atlantic,','Atlantic,','Indian,','Pacific,','Atlantic,','Atlantic,','Pacific,','Atlantic,','Atlantic,']\n",
    "\n",
    "    oliver_data = []\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while i < len(file_mat):\n",
    "        with open(folder_location + file_mat[i]) as f:\n",
    "            for line in f:\n",
    "                oliver_data.append(location[i] + line)\n",
    "        i += 1\n",
    "\n",
    "    df = pd.DataFrame([sub.split(\",\") for sub in oliver_data])\n",
    "\n",
    "    df.columns = ['Location','Core','Lat','Lon','Dep','Core depth','age','Species','pl1','pl2','d18O benthic','d13C']\n",
    "\n",
    "    df = df[['Core','Location','Lat','Lon','Dep','d13C','age']]\n",
    "\n",
    "    df['d13C'] = [i.rstrip() for i in df['d13C']]\n",
    "\n",
    "    df = df[df['age'].astype(float) > float(minn)]\n",
    "    df = df[df['age'].astype(float) < float(maxx)]\n",
    "\n",
    "    ################################# other data                                                                                                                                                                                                                                  \n",
    "\n",
    "    file_mat = ['CH69_K09.txt',\n",
    "    'MD03_2664.txt',\n",
    "    'MD95_2042.txt',\n",
    "    'U1308.txt',\n",
    "    'ODP1063.txt']\n",
    "\n",
    "    locations = ['CH69_K09\\tAtlantic\\t41.75\\t-47.35\\t4100\\t',\n",
    "              'MD03_2664\\tAtlantic\\t57.439000\\t-48.605800\\t3442.0\\t',\n",
    "              'MD95_2042\\tAtlantic\\t37.799833\\t-10.166500\\t3146.0\\t',\n",
    "              'U1308\\tAtlantic\\t49.877760\\t-24.238110\\t3871.0\\t',\n",
    "              'ODP1063\\tAtlantic\\t33.683333\\t-57.616667\\t4584\\t']\n",
    "\n",
    "    other_data = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(file_mat):\n",
    "        with open(folder_location + file_mat[i]) as f:\n",
    "            for line in f:\n",
    "                other_data.append(locations[i]+line)\n",
    "        i += 1\n",
    "\n",
    "    df2 = pd.DataFrame([sub.split(\"\\t\") for sub in other_data])\n",
    "\n",
    "    df2.columns = ['Core','Location','Lat','Lon','Dep','Core Depth','age','d13C']\n",
    "\n",
    "    df2 = df2[['Core','Location','Lat','Lon','Dep','d13C','age']]\n",
    "\n",
    "    df2['d13C'] = [i.rstrip() for i in df2['d13C']]\n",
    "\n",
    "    df2 = df2[df2['age'].astype(float) > float(minn)]\n",
    "    df2 = df2[df2['age'].astype(float) < float(maxx)]\n",
    "\n",
    "    results = pd.concat([df, df2])\n",
    "\n",
    "    results = results[results['d13C'] != '']\n",
    "\n",
    "    results['d13C'] = results['d13C'].astype(float)\n",
    "    results['Lat'] = results['Lat'].astype(float)\n",
    "    results['Lon'] = results['Lon'].astype(float)\n",
    "    results['Dep'] = results['Dep'].astype(float)\n",
    "\n",
    "    return(results)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def pl_cores(minn, maxx):\n",
    "\n",
    "    names = ['Core', 'Location', 'Lat', 'Lon', 'Dep']\n",
    "\n",
    "    # Read in the data\n",
    "    indopac = pd.read_table(\"../Moving_water_mass/Data/Core_files/indopac_core_data_LS16.txt\", delimiter = ',', names = names)\n",
    "    atl = pd.read_table(\"../Moving_water_mass/Data/Core_files/atl_core_data_LS16.txt\", delimiter = ',', names = names)\n",
    "    add = pd.read_table(\"../Moving_water_mass/Data/Core_files/Additional_core_locations.txt\", delimiter = ',', usecols = [0, 1, 2, 3, 4], names = names)\n",
    "\n",
    "    # Join all into a single dataframe\n",
    "    df = indopac.append(atl)\n",
    "    df = df.append(add)\n",
    "    df = df.reset_index(drop = True)\n",
    "\n",
    "    df['d18O names'] = df['Core'] + '_ageLS16.txt'\n",
    "    df['d13C names'] = df['Core'] + '_d13C.txt'\n",
    "\n",
    "    # Loop over the dataset and interpolate each core\n",
    "    i = 0\n",
    "    results_dict = {}\n",
    "\n",
    "    while i < df.count()[0]:\n",
    "\n",
    "        try:\n",
    "            df_d18O = pd.read_table('../Moving_water_mass/Data/Core_files/' + df.loc[i]['d18O names'], delim_whitespace = True, names = ['depth', 'age'], skip_blank_lines = True, na_values = 'NAN')\n",
    "        except:\n",
    "            i += 1\n",
    "            continue\n",
    "        try:\n",
    "            df_d13C = pd.read_table('../Moving_water_mass/Data/Core_files/' + df.loc[i]['d13C names'], delim_whitespace = True, names = ['depth', 'd13C'], skip_blank_lines = True, na_values = 'NAN')\n",
    "        except:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        df_d18O = df_d18O.dropna(subset = ['age']) \n",
    "        df_d13C = df_d13C.dropna(subset = ['d13C'])\n",
    "\n",
    "        df_d18O = df_d18O.reset_index(drop = True)\n",
    "        df_d13C = df_d13C.reset_index(drop = True)\n",
    "\n",
    "        interp = scipy.interpolate.interp1d(df_d18O['depth'], df_d18O['age'], bounds_error = True)\n",
    "        try:\n",
    "            df_d13C['age'] = interp(df_d13C['depth'])\n",
    "        except:\n",
    "            try:\n",
    "                interp2 = scipy.interpolate.interp1d(df_d18O['depth'], df_d18O['age'], bounds_error = False)\n",
    "                df_d13C['age'] = interp2(df_d13C['depth'])\n",
    "            except:\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        df_d13C = df_d13C.dropna(subset = ['age'])\n",
    "        df_d13C = df_d13C.reset_index(drop = True)\n",
    "\n",
    "        df_d13C = df_d13C[df_d13C['age'] > minn]\n",
    "        df_d13C = df_d13C[df_d13C['age'] < maxx]\n",
    "\n",
    "        if len(df_d13C) > 0:\n",
    "            df_results = df.drop(['d18O names', 'd13C names'], axis = 1)\n",
    "            df_results = df_results.loc[df_results.index.repeat(len(df_d13C))].loc[[i]]\n",
    "\n",
    "            df_d13C = df_d13C.drop(['depth'], axis = 1)\n",
    "\n",
    "            df_results = df_results.reset_index(drop = True).join(df_d13C.reset_index(drop = True))\n",
    "            results_dict.update({\n",
    "                df_results.Core[0] : df_results.drop(['Core'], axis = 1)\n",
    "            })\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "    df_results = pd.concat(results_dict).reset_index()\n",
    "    df_results = df_results.rename(columns = {'level_0' : 'Core'})\n",
    "    df_results = df_results.drop(['level_1'], axis = 1)\n",
    "\n",
    "    return(df_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define variables\n",
    "\n",
    "Hol_min = 2\n",
    "Hol_max = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atlantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Holocene Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z5145948/anaconda3/lib/python3.6/site-packages/pandas/compat/_optional.py:106: UserWarning: Pandas requires version '2.6.2' or newer of 'numexpr' (version '2.6.1' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# ################## read in the oliver data using the predefined function\n",
    "\n",
    "# Age range to look over\n",
    "\n",
    "df_pl = pl_cores(Hol_min,Hol_max)\n",
    "\n",
    "df_oliver = Oliver_cores(Hol_min,Hol_max)\n",
    "\n",
    "###################3 join the dataframes\n",
    "\n",
    "df_results = df_pl.append(df_oliver)\n",
    "  \n",
    "#################### get only the atlantic cores\n",
    "\n",
    "df = df_results.reset_index(drop = True)\n",
    "df['Dep'] = abs(df['Dep'])\n",
    "\n",
    "df_atl = df[df['Location'] == 'Atlantic']\n",
    "df_atl = df_atl.reset_index(drop = True)\n",
    "df_atl['age'] = df_atl.age.astype(float)\n",
    "\n",
    "# Slice the data\n",
    "lower = np.arange(Hol_min, Hol_max, slice_width)\n",
    "upper = np.arange(Hol_min+slice_width, Hol_max+slice_width, slice_width)\n",
    "\n",
    "proxy_compilation = {}\n",
    "\n",
    "for low, up in zip(lower, upper):\n",
    "    df_slice = df_atl[(df_atl['age'] >= low) & (df_atl['age'] < up)]\n",
    "    grouped_slice = df_slice.groupby(['Lat', 'Lon', 'Dep'])\n",
    "\n",
    "    slice_averaged = {}\n",
    "\n",
    "    for key, group in grouped_slice:\n",
    "        group_averaged = group.mean()\n",
    "        group_count = group.count()\n",
    "        group_averaged['count'] = group_count['d13C']\n",
    "        slice_averaged.update({\n",
    "            group.Core.reset_index(drop=True)[0] : pd.DataFrame(group_averaged)\n",
    "        })\n",
    "    \n",
    "    slice_averaged = pd.concat(slice_averaged, axis=1).T\n",
    "    slice_averaged = slice_averaged.drop(['age'],axis=1)\n",
    "    \n",
    "    proxy_compilation.update({\n",
    "        low : slice_averaged.T\n",
    "    })\n",
    "\n",
    "proxy_compilation = pd.concat(proxy_compilation,axis=1).T.reset_index(drop=False)\n",
    "proxy_compilation = proxy_compilation.drop('level_2',axis=1)\n",
    "proxy_compilation = proxy_compilation.rename(columns={'level_0' : 'lower', 'level_1' : 'Core', 'Dep' : 'Ocean_depth'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save list of cores to latex table to include in paper\n",
    "latex_table_Hol = proxy_compilation.drop(['d13C','count','lower'],axis=1)\n",
    "\n",
    "latex_table_Hol = latex_table_Hol.drop_duplicates()\n",
    "\n",
    "proxy_compilation.drop('Core',axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# interpolating across the entire dataset\n",
    "\n",
    "# Add time bounds to the samples table\n",
    "samples_with_time_period = proxy_compilation\n",
    "\n",
    "unique_lats = np.unique(samples_with_time_period['Lat'])\n",
    "years_list = np.unique(samples_with_time_period.lower)\n",
    "\n",
    "samples_with_time_period = samples_with_time_period.reset_index(drop=True)\n",
    "\n",
    "interpolated_proxies = {}\n",
    "\n",
    "for unique_lat in unique_lats:\n",
    "    \n",
    "    # get a single proxy\n",
    "    df_temp = samples_with_time_period[samples_with_time_period['Lat'] == unique_lat]\n",
    "\n",
    "    try:\n",
    "        # interpolate the dataset\n",
    "        interp = scipy.interpolate.interp1d(df_temp['lower'],\n",
    "                                        df_temp['d13C'],\n",
    "                                        bounds_error = False)\n",
    "    except ValueError:\n",
    "        interpolated_proxies.update({\n",
    "            unique_lat : df_temp\n",
    "        })\n",
    "    \n",
    "    single_proxy_interpolated = pd.DataFrame({'lower' : years_list, 'd13C' : interp(years_list)})\n",
    "    \n",
    "    location_repeated = pd.concat([df_temp.reset_index(0).loc[0,['Lat', 'Lon', 'Ocean_depth']]] * len(single_proxy_interpolated), axis=1).T\n",
    "    single_proxy_interpolated = pd.concat([location_repeated.reset_index(drop=True), single_proxy_interpolated.reset_index(drop=True)],axis=1)    \n",
    "    \n",
    "    interpolated_proxies.update({\n",
    "        unique_lat : single_proxy_interpolated\n",
    "    })\n",
    "    \n",
    "interpolated_samples = pd.concat(interpolated_proxies).reset_index(drop=True)\n",
    "interpolated_samples_backup = interpolated_samples.copy()\n",
    "\n",
    "# Combine the interpolated and not interpolated dataframes together so that\n",
    "\n",
    "interpolated_samples = pd.merge(\n",
    "    proxy_compilation, interpolated_samples, on=['Lat','Lon','Ocean_depth','d13C','lower'],\n",
    "    indicator=True,how='right'\n",
    ")\n",
    "\n",
    "# Replace the keys 'both' or 'right_only' with colors, to set scatter plot fill\n",
    "interpolated_samples.replace('right_only','none',inplace=True)\n",
    "\n",
    "shallower_samples = interpolated_samples[interpolated_samples['Ocean_depth'] < dividing_depth].replace('both',str(point_color_shallow))\n",
    "deeper_samples = interpolated_samples[interpolated_samples['Ocean_depth'] > dividing_depth].replace('both',str(point_color_deep))\n",
    "\n",
    "interpolated_samples = shallower_samples.append(deeper_samples)\n",
    "edgecolor = [point_color_deep if x >= dividing_depth else point_color_shallow for x in interpolated_samples.Ocean_depth]\n",
    "interpolated_samples['edgecolor'] = edgecolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get volume weight average with time slices\n",
    "\n",
    "interpolated_samples['weights'] = np.nan\n",
    "\n",
    "# NEA\n",
    "interpolated_samples.loc[((interpolated_samples['Lon'] < 20) | (interpolated_samples['Lon'] > (-33))) & (interpolated_samples['Lat'] > 0.1),\n",
    "                        'weights'] = 4.3\n",
    "\n",
    "# NWA\n",
    "interpolated_samples.loc[(interpolated_samples['Lon'] < (-33)) & (interpolated_samples['Lon'] > (-180)) & (interpolated_samples['Lat'] > 0.1),\n",
    "                         'weights'] = 4.9\n",
    "\n",
    "# SEA\n",
    "interpolated_samples.loc[((interpolated_samples['Lon'] < 30) | (interpolated_samples['Lon'] > (-14.6))) & (interpolated_samples['Lat'] < 0) & (interpolated_samples['Lat'] > -55),\n",
    "                         'weights'] = 3.5\n",
    "\n",
    "# SA\n",
    "interpolated_samples.loc[((interpolated_samples['Lon'] < 30) | (interpolated_samples['Lon'] > (-22))) & (interpolated_samples['Lat'] < -40) & (interpolated_samples['Lat'] > -55),\n",
    "                        'weights'] = 0.7\n",
    "\n",
    "# SWA\n",
    "interpolated_samples.loc[(interpolated_samples['Lon'] > (-60)) & (interpolated_samples['Lon'] < (-14.6)) & (interpolated_samples['Lat'] < 0) & (interpolated_samples['Lat'] > -55),\n",
    "                        'weights'] = 5.0\n",
    "\n",
    "# group the cores based on the age and the region (indicated by weights)\n",
    "grouped_by_age_region = interpolated_samples.groupby(['lower', 'weights'])\n",
    "\n",
    "averaged_by_age_region = {}\n",
    "\n",
    "# find group means\n",
    "for key, group in grouped_by_age_region:\n",
    "    averaged_by_age_region.update({\n",
    "        key: np.mean(group)\n",
    "    })\n",
    "    \n",
    "averaged_by_age_region = pd.concat(averaged_by_age_region,axis=1).T.reset_index(drop=True)\n",
    "\n",
    "# Now there is one values for each region (weight) and each year combination\n",
    "# Group by years and use weights to find the average d13C for that time period\n",
    "\n",
    "grouped_by_age = averaged_by_age_region.groupby('lower')\n",
    "\n",
    "averaged_by_age = {}\n",
    "stdev_by_age = {}\n",
    "\n",
    "# find group means\n",
    "for key, group in grouped_by_age:\n",
    "    \n",
    "    # Find the normal average\n",
    "    avg = np.mean(group.d13C)\n",
    "    \n",
    "    averaged_by_age.update({\n",
    "        np.mean(group['lower']) : np.sum(group['d13C'] * group['weights'])/np.sum(group['weights'])\n",
    "    })\n",
    "    stdev_by_age.update({\n",
    "        np.mean(group['lower']) : np.average((group.d13C-avg)**2,weights=group.weights)\n",
    "    }) \n",
    "    \n",
    "    \n",
    "averaged_by_age = pd.DataFrame.from_dict(averaged_by_age,orient='index').reset_index(drop=False).rename(columns={'index' : 'lower', 0 : 'd13C'})\n",
    "stdev_by_age = pd.DataFrame.from_dict(stdev_by_age,orient='index').reset_index(drop=False).rename(columns={'index' : 'lower', 0 : 'stdev'})\n",
    "\n",
    "averaged_by_age = pd.merge(left=averaged_by_age,right=stdev_by_age)\n",
    "averaged_by_age = averaged_by_age.sort_values('lower')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy the Holocene datasets to new variables\n",
    "averaged_by_age_Holocene_Atlantic = averaged_by_age.copy()\n",
    "interpolated_samples_Holocene_Atlantic = interpolated_samples.copy()\n",
    "\n",
    "interpolated_samples = interpolated_samples.drop('weights',axis=1)\n",
    "# Save the datasets\n",
    "averaged_by_age.to_csv(\"Data/Atlantic_Holocene_profile_PL.csv\", index=False)\n",
    "interpolated_samples.to_csv(\"/srv/ccrc/data06/z5145948/Moving_water_mass/Data/interpolated_Hol_Atlantic.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIG Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ################## read in the oliver data using the predefined function\n",
    "\n",
    "# Age range to look over\n",
    "LIG_min = 118\n",
    "LIG_max = 130\n",
    "\n",
    "df_pl = pl_cores(LIG_min,LIG_max)\n",
    "\n",
    "df_oliver = Oliver_cores(LIG_min,LIG_max)\n",
    "\n",
    "###################3 join the dataframes\n",
    "\n",
    "df_results = df_pl.append(df_oliver)\n",
    "  \n",
    "#################### get only the atlantic cores\n",
    "\n",
    "df = df_results.reset_index(drop = True)\n",
    "df['Dep'] = abs(df['Dep'])\n",
    "\n",
    "df_atl = df[df['Location'] == 'Atlantic']\n",
    "df_atl = df_atl.reset_index(drop = True)\n",
    "df_atl['age'] = df_atl.age.astype(float)\n",
    "\n",
    "# Slice the data\n",
    "lower = np.arange(LIG_min, LIG_max, slice_width)\n",
    "upper = np.arange(LIG_min+slice_width, LIG_max+slice_width, slice_width)\n",
    "\n",
    "proxy_compilation = {}\n",
    "\n",
    "for low, up in zip(lower, upper):\n",
    "    df_slice = df_atl[(df_atl['age'] >= low) & (df_atl['age'] < up)]\n",
    "    grouped_slice = df_slice.groupby(['Lat', 'Lon', 'Dep'])\n",
    "\n",
    "    slice_averaged = {}\n",
    "\n",
    "    for key, group in grouped_slice:\n",
    "        group_averaged = group.mean()\n",
    "        group_count = group.count()\n",
    "        group_averaged['count'] = group_count['d13C']\n",
    "        slice_averaged.update({\n",
    "            group.Core.reset_index(drop=True)[0] : pd.DataFrame(group_averaged)\n",
    "        })\n",
    "    \n",
    "    slice_averaged = pd.concat(slice_averaged, axis=1).T\n",
    "    slice_averaged = slice_averaged.drop(['age'],axis=1)\n",
    "    \n",
    "    proxy_compilation.update({\n",
    "        low : slice_averaged.T\n",
    "    })\n",
    "\n",
    "proxy_compilation = pd.concat(proxy_compilation,axis=1).T.reset_index(drop=False)\n",
    "proxy_compilation = proxy_compilation.drop('level_2',axis=1)\n",
    "proxy_compilation = proxy_compilation.rename(columns={'level_0' : 'lower', 'level_1' : 'Core', 'Dep' : 'Ocean_depth'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save list of cores to latex table to include in paper\n",
    "latex_table_LIG = proxy_compilation.drop(['d13C','count','lower'],axis=1)\n",
    "latex_table_LIG = latex_table_LIG.drop_duplicates()\n",
    "\n",
    "proxy_compilation.drop('Core',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Combine the two latex tables\n",
    "latex_table = latex_table_Hol.merge(latex_table_LIG,on=['Core','Lat','Lon','Ocean_depth'],how='outer',indicator=True)\n",
    "\n",
    "# Format the table\n",
    "latex_table.replace(to_replace='both',value='Holocene & LIG',inplace=True)\n",
    "latex_table.replace(to_replace='right_only',value='LIG',inplace=True)\n",
    "latex_table.replace(to_replace='left_only',value='Holocene',inplace=True)\n",
    "latex_table = latex_table.rename(columns={'Lat':'Latitude','Lon' : 'Longitude', 'Ocean_depth' : 'Depth (m)', '_merge':'Time Period'})\n",
    "latex_table['Latitude'] = [str(round(x, 2)) for x in latex_table.Latitude]\n",
    "latex_table['Longitude'] = [str(round(x, 2)) for x in latex_table.Longitude]\n",
    "latex_table.sort_values(by='Core',inplace=True)\n",
    "\n",
    "# Convert to string of latex markdown\n",
    "latex_string = latex_table.to_latex(index=False,longtable=True)\n",
    "\n",
    "# Reformat some parts of the latex table\n",
    "latex_string = latex_string.replace('\\\\toprule','')\n",
    "latex_string = latex_string.replace('\\\\midrule','')\n",
    "latex_string = latex_string.replace('\\\\bottomrule','')\n",
    "\n",
    "# Add caption to latex table\n",
    "caption = [\n",
    "    '\\caption{Latitude, Longitude, and depth (m) coordinates for the Atlantic benthic foraminifera',\n",
    "    '$\\delta^{13}$C cores. \\\\textit{Time Period} refers to whether this core had',\n",
    "    'Holocene, Last Interglacial, or Holocene and Last Interglacial data.}'\n",
    "]\n",
    "\n",
    "caption = ' '.join(caption)\n",
    "\n",
    "latex_string = latex_string.replace('\\\\end{longtable}',caption+'\\\\end{longtable}')\n",
    "\n",
    "# Write to a file\n",
    "file1 = open(\"Figures/Cores_tables_Atlantic.tex\",\"w\") \n",
    "file1.write(latex_string) \n",
    "file1.close() #to change file access modes \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# interpolating across the entire dataset\n",
    "\n",
    "# Add time bounds to the samples table\n",
    "samples_with_time_period = proxy_compilation\n",
    "\n",
    "unique_lats = np.unique(samples_with_time_period['Lat'])\n",
    "years_list = np.unique(samples_with_time_period.lower)\n",
    "\n",
    "samples_with_time_period = samples_with_time_period.reset_index(drop=True)\n",
    "\n",
    "interpolated_proxies = {}\n",
    "\n",
    "for unique_lat in unique_lats:\n",
    "    \n",
    "    # get a single proxy\n",
    "    df_temp = samples_with_time_period[samples_with_time_period['Lat'] == unique_lat]\n",
    "\n",
    "    try:\n",
    "        # interpolate the dataset\n",
    "        interp = scipy.interpolate.interp1d(df_temp['lower'],\n",
    "                                        df_temp['d13C'],\n",
    "                                        bounds_error = False)\n",
    "    except ValueError:\n",
    "        interpolated_proxies.update({\n",
    "            unique_lat : df_temp\n",
    "        })\n",
    "    \n",
    "    single_proxy_interpolated = pd.DataFrame({'lower' : years_list, 'd13C' : interp(years_list)})\n",
    "    \n",
    "    location_repeated = pd.concat([df_temp.reset_index(0).loc[0,['Lat', 'Lon', 'Ocean_depth']]] * len(single_proxy_interpolated), axis=1).T\n",
    "    single_proxy_interpolated = pd.concat([location_repeated.reset_index(drop=True), single_proxy_interpolated.reset_index(drop=True)],axis=1)    \n",
    "    \n",
    "    interpolated_proxies.update({\n",
    "        unique_lat : single_proxy_interpolated\n",
    "    })\n",
    "    \n",
    "interpolated_samples = pd.concat(interpolated_proxies).reset_index(drop=True)\n",
    "interpolated_samples_backup = interpolated_samples.copy()\n",
    "\n",
    "# Combine the interpolated and not interpolated dataframes together so that\n",
    "\n",
    "interpolated_samples = pd.merge(\n",
    "    proxy_compilation, interpolated_samples, on=['Lat','Lon','Ocean_depth','d13C','lower'],\n",
    "    indicator=True,how='right'\n",
    ")\n",
    "\n",
    "# Replace the keys 'both' or 'right_only' with colors, to set scatter plot fill\n",
    "interpolated_samples.replace('right_only','none',inplace=True)\n",
    "\n",
    "shallower_samples = interpolated_samples[interpolated_samples['Ocean_depth'] < dividing_depth].replace('both',str(point_color_shallow))\n",
    "deeper_samples = interpolated_samples[interpolated_samples['Ocean_depth'] > dividing_depth].replace('both',str(point_color_deep))\n",
    "\n",
    "interpolated_samples = shallower_samples.append(deeper_samples)\n",
    "edgecolor = [point_color_deep if x >= dividing_depth else point_color_shallow for x in interpolated_samples.Ocean_depth]\n",
    "interpolated_samples['edgecolor'] = edgecolor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get volume weight average with time slices\n",
    "\n",
    "interpolated_samples['weights'] = np.nan\n",
    "\n",
    "# NEA\n",
    "interpolated_samples.loc[((interpolated_samples['Lon'] < 20) | (interpolated_samples['Lon'] > (-33))) & (interpolated_samples['Lat'] > 0.1),\n",
    "                        'weights'] = 4.3\n",
    "\n",
    "# NWA\n",
    "interpolated_samples.loc[(interpolated_samples['Lon'] < (-33)) & (interpolated_samples['Lon'] > (-180)) & (interpolated_samples['Lat'] > 0.1),\n",
    "                         'weights'] = 4.9\n",
    "\n",
    "# SEA\n",
    "interpolated_samples.loc[((interpolated_samples['Lon'] < 30) | (interpolated_samples['Lon'] > (-14.6))) & (interpolated_samples['Lat'] < 0) & (interpolated_samples['Lat'] > -55),\n",
    "                         'weights'] = 3.5\n",
    "\n",
    "# SA\n",
    "interpolated_samples.loc[((interpolated_samples['Lon'] < 30) | (interpolated_samples['Lon'] > (-22))) & (interpolated_samples['Lat'] < -40) & (interpolated_samples['Lat'] > -55),\n",
    "                        'weights'] = 0.7\n",
    "\n",
    "# SWA\n",
    "interpolated_samples.loc[(interpolated_samples['Lon'] > (-60)) & (interpolated_samples['Lon'] < (-14.6)) & (interpolated_samples['Lat'] < 0) & (interpolated_samples['Lat'] > -55),\n",
    "                        'weights'] = 5.0\n",
    "\n",
    "# group the cores based on the age and the region (indicated by weights)\n",
    "grouped_by_age_region = interpolated_samples.groupby(['lower', 'weights'])\n",
    "\n",
    "averaged_by_age_region = {}\n",
    "\n",
    "# find group means\n",
    "for key, group in grouped_by_age_region:\n",
    "    averaged_by_age_region.update({\n",
    "        key: np.mean(group)\n",
    "    })\n",
    "    \n",
    "averaged_by_age_region = pd.concat(averaged_by_age_region,axis=1).T.reset_index(drop=True)\n",
    "\n",
    "# Now there is one values for each region (weight) and each year combination\n",
    "# Group by years and use weights to find the average d13C for that time period\n",
    "\n",
    "grouped_by_age = averaged_by_age_region.groupby('lower')\n",
    "\n",
    "averaged_by_age = {}\n",
    "stdev_by_age = {}\n",
    "\n",
    "# find group means\n",
    "for key, group in grouped_by_age:\n",
    "    \n",
    "    # Find the normal average\n",
    "    avg = np.mean(group.d13C)\n",
    "    \n",
    "    averaged_by_age.update({\n",
    "        np.mean(group['lower']) : np.sum(group['d13C'] * group['weights'])/np.sum(group['weights'])\n",
    "    })\n",
    "    stdev_by_age.update({\n",
    "        np.mean(group['lower']) : np.average((group.d13C-avg)**2,weights=group.weights)\n",
    "    }) \n",
    "    \n",
    "    \n",
    "averaged_by_age = pd.DataFrame.from_dict(averaged_by_age,orient='index').reset_index(drop=False).rename(columns={'index' : 'lower', 0 : 'd13C'})\n",
    "stdev_by_age = pd.DataFrame.from_dict(stdev_by_age,orient='index').reset_index(drop=False).rename(columns={'index' : 'lower', 0 : 'stdev'})\n",
    "\n",
    "averaged_by_age = pd.merge(left=averaged_by_age,right=stdev_by_age)\n",
    "averaged_by_age = averaged_by_age.sort_values('lower')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy the Holocene datasets to new variables\n",
    "averaged_by_age_LIG_Atlantic = averaged_by_age.copy()\n",
    "interpolated_samples_LIG_Atlantic = interpolated_samples.copy()\n",
    "\n",
    "interpolated_samples = interpolated_samples.drop('weights',axis=1)\n",
    "# Save the datasets\n",
    "averaged_by_age.to_csv(\"Data/Atlantic_LIG_profile_PL.csv\", index=False)\n",
    "interpolated_samples.to_csv(\"/srv/ccrc/data06/z5145948/Moving_water_mass/Data/interpolated_LIG_Atlantic.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pacific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holocene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ################## read in the oliver data using the predefined function\n",
    "\n",
    "df_pl = pl_cores(Hol_min,Hol_max)\n",
    "\n",
    "df_oliver = Oliver_cores(Hol_min,Hol_max)\n",
    "\n",
    "###################3 join the dataframes\n",
    "\n",
    "df_results = df_pl.append(df_oliver)\n",
    "  \n",
    "#################### get only the pacific cores\n",
    "\n",
    "df = df_results.reset_index(drop = True)\n",
    "df['Dep'] = abs(df['Dep'])\n",
    "\n",
    "df_pac = df[df['Location'] == 'Pacific']\n",
    "df_pac = df_pac.reset_index(drop = True)\n",
    "df_pac['age'] = df_pac.age.astype(float)\n",
    "\n",
    "# Slice the data\n",
    "lower = np.arange(Hol_min, Hol_max, slice_width)\n",
    "upper = np.arange(Hol_min+slice_width, Hol_max+slice_width, slice_width)\n",
    "\n",
    "proxy_compilation = {}\n",
    "\n",
    "for low, up in zip(lower, upper):\n",
    "    df_slice = df_pac[(df_pac['age'] >= low) & (df_pac['age'] < up)]\n",
    "    grouped_slice = df_slice.groupby(['Lat', 'Lon', 'Dep'])\n",
    "\n",
    "    slice_averaged = {}\n",
    "\n",
    "    for key, group in grouped_slice:\n",
    "        group_averaged = group.mean()\n",
    "        group_count = group.count()\n",
    "        group_averaged['count'] = group_count['d13C']\n",
    "        slice_averaged.update({\n",
    "            group.Core.reset_index(drop=True)[0] : pd.DataFrame(group_averaged)\n",
    "        })\n",
    "    \n",
    "    slice_averaged = pd.concat(slice_averaged, axis=1).T\n",
    "    slice_averaged = slice_averaged.drop(['age'],axis=1)\n",
    "    \n",
    "    proxy_compilation.update({\n",
    "        low : slice_averaged.T\n",
    "    })\n",
    "\n",
    "proxy_compilation = pd.concat(proxy_compilation,axis=1).T.reset_index(drop=False)\n",
    "proxy_compilation = proxy_compilation.drop('level_2',axis=1)\n",
    "proxy_compilation = proxy_compilation.rename(columns={'level_0' : 'lower', 'level_1' : 'Core', 'Dep' : 'Ocean_depth'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save list of cores to latex table to include in paper\n",
    "latex_table_Hol = proxy_compilation.drop(['d13C','count','lower'],axis=1)\n",
    "\n",
    "latex_table_Hol = latex_table_Hol.drop_duplicates()\n",
    "\n",
    "proxy_compilation.drop('Core',axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z5145948/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:43: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# interpolating across the entire dataset\n",
    "\n",
    "# Add time bounds to the samples table\n",
    "samples_with_time_period = proxy_compilation\n",
    "\n",
    "unique_lats = np.unique(samples_with_time_period['Lat'])\n",
    "years_list = np.unique(samples_with_time_period.lower)\n",
    "\n",
    "samples_with_time_period = samples_with_time_period.reset_index(drop=True)\n",
    "\n",
    "interpolated_proxies = {}\n",
    "\n",
    "for unique_lat in unique_lats:\n",
    "    \n",
    "    # get a single proxy\n",
    "    df_temp = samples_with_time_period[samples_with_time_period['Lat'] == unique_lat]\n",
    "    \n",
    "#     print(df_temp)\n",
    "\n",
    "    try:\n",
    "        # interpolate the dataset\n",
    "        \n",
    "#         embed()\n",
    "        \n",
    "        interp = scipy.interpolate.interp1d(df_temp['lower'],\n",
    "                                        df_temp['d13C'],\n",
    "                                        bounds_error = False)\n",
    "        single_proxy_interpolated = pd.DataFrame({'lower' : years_list, 'd13C' : interp(years_list)})\n",
    "\n",
    "        location_repeated = pd.concat([df_temp.reset_index(0).loc[0,['Lat', 'Lon', 'Ocean_depth']]] * len(single_proxy_interpolated), axis=1).T\n",
    "        single_proxy_interpolated = pd.concat([location_repeated.reset_index(drop=True), single_proxy_interpolated.reset_index(drop=True)],axis=1)    \n",
    "\n",
    "        interpolated_proxies.update({\n",
    "            unique_lat : single_proxy_interpolated\n",
    "        })\n",
    "        \n",
    "    except ValueError:\n",
    "\n",
    "        interpolated_proxies.update({\n",
    "            unique_lat : df_temp.drop('count',axis=1)\n",
    "        })\n",
    "    \n",
    "interpolated_samples = pd.concat(interpolated_proxies).reset_index(drop=True)\n",
    "interpolated_samples_backup = interpolated_samples.copy()\n",
    "\n",
    "# Combine the interpolated and not interpolated dataframes together so that\n",
    "\n",
    "interpolated_samples = pd.merge(\n",
    "    proxy_compilation, interpolated_samples, on=['Lat','Lon','Ocean_depth','d13C','lower'],\n",
    "    indicator=True,how='right'\n",
    ")\n",
    "\n",
    "# Replace the keys 'both' or 'right_only' with colors, to set scatter plot fill\n",
    "interpolated_samples.replace('right_only','none',inplace=True)\n",
    "\n",
    "shallower_samples = interpolated_samples[interpolated_samples['Ocean_depth'] < dividing_depth].replace('both',str(point_color_shallow))\n",
    "deeper_samples = interpolated_samples[interpolated_samples['Ocean_depth'] > dividing_depth].replace('both',str(point_color_deep))\n",
    "\n",
    "interpolated_samples = shallower_samples.append(deeper_samples)\n",
    "edgecolor = [point_color_deep if x >= dividing_depth else point_color_shallow for x in interpolated_samples.Ocean_depth]\n",
    "interpolated_samples['edgecolor'] = edgecolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get volume weight average with time slices\n",
    "\n",
    "interpolated_samples['weights'] = np.nan\n",
    "\n",
    "# North\n",
    "interpolated_samples.loc[(interpolated_samples['Lat'] > 0) ,'weights'] = 21.2\n",
    "\n",
    "# South\n",
    "interpolated_samples.loc[(interpolated_samples['Lat'] < 0) ,'weights'] = 23.9\n",
    "\n",
    "# group the cores based on the age and the region (indicated by weights)\n",
    "grouped_by_age_region = interpolated_samples.groupby(['lower', 'weights'])\n",
    "\n",
    "averaged_by_age_region = {}\n",
    "\n",
    "# find group means\n",
    "for key, group in grouped_by_age_region:\n",
    "    averaged_by_age_region.update({\n",
    "        key: np.mean(group)\n",
    "    })\n",
    "    \n",
    "averaged_by_age_region = pd.concat(averaged_by_age_region,axis=1).T.reset_index(drop=True)\n",
    "\n",
    "# Now there is one values for each region (weight) and each year combination\n",
    "# Group by years and use weights to find the average d13C for that time period\n",
    "\n",
    "grouped_by_age = averaged_by_age_region.groupby('lower')\n",
    "\n",
    "averaged_by_age = {}\n",
    "stdev_by_age = {}\n",
    "\n",
    "# find group means\n",
    "for key, group in grouped_by_age:\n",
    "    \n",
    "    # Find the normal average\n",
    "    avg = np.mean(group.d13C)\n",
    "    \n",
    "    averaged_by_age.update({\n",
    "        np.mean(group['lower']) : np.sum(group['d13C'] * group['weights'])/np.sum(group['weights'])\n",
    "    })\n",
    "    stdev_by_age.update({\n",
    "        np.mean(group['lower']) : np.average((group.d13C-avg)**2,weights=group.weights)\n",
    "    }) \n",
    "    \n",
    "    \n",
    "averaged_by_age = pd.DataFrame.from_dict(averaged_by_age,orient='index').reset_index(drop=False).rename(columns={'index' : 'lower', 0 : 'd13C'})\n",
    "stdev_by_age = pd.DataFrame.from_dict(stdev_by_age,orient='index').reset_index(drop=False).rename(columns={'index' : 'lower', 0 : 'stdev'})\n",
    "\n",
    "averaged_by_age = pd.merge(left=averaged_by_age,right=stdev_by_age)\n",
    "averaged_by_age = averaged_by_age.sort_values('lower')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy the Holocene datasets to new variables\n",
    "averaged_by_age_Holocene_Pacific = averaged_by_age.copy()\n",
    "interpolated_samples_Holocene_Pacific = interpolated_samples.copy()\n",
    "\n",
    "interpolated_samples = interpolated_samples.drop('weights',axis=1)\n",
    "# Save the datasets\n",
    "averaged_by_age.to_csv(\"Data/Pacific_Holocene_profile_PL.csv\", index=False)\n",
    "interpolated_samples.to_csv(\"/srv/ccrc/data06/z5145948/Moving_water_mass/Data/interpolated_Hol_Pacific.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pl = pl_cores(LIG_min,LIG_max)\n",
    "\n",
    "df_oliver = Oliver_cores(LIG_min,LIG_max)\n",
    "\n",
    "###################3 join the dataframes\n",
    "\n",
    "df_results = df_pl.append(df_oliver)\n",
    "  \n",
    "#################### get only the pacantic cores\n",
    "\n",
    "df = df_results.reset_index(drop = True)\n",
    "df['Dep'] = abs(df['Dep'])\n",
    "\n",
    "df_pac = df[df['Location'] == 'Pacific']\n",
    "df_pac = df_pac.reset_index(drop = True)\n",
    "df_pac['age'] = df_pac.age.astype(float)\n",
    "\n",
    "# Slice the data\n",
    "lower = np.arange(LIG_min, LIG_max, slice_width)\n",
    "upper = np.arange(LIG_min+slice_width, LIG_max+slice_width, slice_width)\n",
    "\n",
    "proxy_compilation = {}\n",
    "\n",
    "for low, up in zip(lower, upper):\n",
    "    df_slice = df_pac[(df_pac['age'] >= low) & (df_pac['age'] < up)]\n",
    "    grouped_slice = df_slice.groupby(['Lat', 'Lon', 'Dep'])\n",
    "\n",
    "    slice_averaged = {}\n",
    "\n",
    "    for key, group in grouped_slice:\n",
    "        group_averaged = group.mean()\n",
    "        group_count = group.count()\n",
    "        group_averaged['count'] = group_count['d13C']\n",
    "        slice_averaged.update({\n",
    "            group.Core.reset_index(drop=True)[0] : pd.DataFrame(group_averaged)\n",
    "        })\n",
    "    \n",
    "    slice_averaged = pd.concat(slice_averaged, axis=1).T\n",
    "    slice_averaged = slice_averaged.drop(['age'],axis=1)\n",
    "    \n",
    "    proxy_compilation.update({\n",
    "        low : slice_averaged.T\n",
    "    })\n",
    "\n",
    "proxy_compilation = pd.concat(proxy_compilation,axis=1).T.reset_index(drop=False)\n",
    "proxy_compilation = proxy_compilation.drop('level_2',axis=1)\n",
    "proxy_compilation = proxy_compilation.rename(columns={'level_0' : 'lower', 'level_1' : 'Core', 'Dep' : 'Ocean_depth'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save list of cores to latex table to include in paper\n",
    "latex_table_LIG = proxy_compilation.drop(['d13C','count','lower'],axis=1)\n",
    "latex_table_LIG = latex_table_LIG.drop_duplicates()\n",
    "\n",
    "proxy_compilation.drop('Core',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine the two latex tables\n",
    "latex_table = latex_table_Hol.merge(latex_table_LIG,on=['Core','Lat','Lon','Ocean_depth'],how='outer',indicator=True)\n",
    "\n",
    "# Format the table\n",
    "latex_table.replace(to_replace='both',value='Holocene & LIG',inplace=True)\n",
    "latex_table.replace(to_replace='right_only',value='LIG',inplace=True)\n",
    "latex_table.replace(to_replace='left_only',value='Holocene',inplace=True)\n",
    "latex_table = latex_table.rename(columns={'Lat':'Latitude','Lon' : 'Longitude', 'Ocean_depth' : 'Depth (m)', '_merge':'Time Period'})\n",
    "latex_table['Latitude'] = [str(round(x, 2)) for x in latex_table.Latitude]\n",
    "latex_table['Longitude'] = [str(round(x, 2)) for x in latex_table.Longitude]\n",
    "latex_table.sort_values(by='Core',inplace=True)\n",
    "\n",
    "# Convert to string of latex markdown\n",
    "latex_string = latex_table.to_latex(index=False,longtable=True)\n",
    "\n",
    "# Reformat some parts of the latex table\n",
    "latex_string = latex_string.replace('\\\\toprule','')\n",
    "latex_string = latex_string.replace('\\\\midrule','')\n",
    "latex_string = latex_string.replace('\\\\bottomrule','')\n",
    "\n",
    "# Add caption to latex table\n",
    "caption = [\n",
    "    '\\caption{Latitude, Longitude, and depth (m) coordinates for the Pacific benthic foraminifera',\n",
    "    '$\\delta^{13}$C cores. \\\\textit{Time Period} refers to whether this core had',\n",
    "    'Holocene, Last Interglacial, or Holocene and Last Interglacial data.}'\n",
    "]\n",
    "\n",
    "caption = ' '.join(caption)\n",
    "\n",
    "latex_string = latex_string.replace('\\\\end{longtable}',caption+'\\\\end{longtable}')\n",
    "\n",
    "# Write to a file\n",
    "file1 = open(\"Figures/Cores_tables_Pacific.tex\",\"w\") \n",
    "file1.write(latex_string) \n",
    "file1.close() #to change file access modes \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# interpolating across the entire dataset\n",
    "\n",
    "# Add time bounds to the samples table\n",
    "samples_with_time_period = proxy_compilation\n",
    "\n",
    "unique_lats = np.unique(samples_with_time_period['Lat'])\n",
    "years_list = np.unique(samples_with_time_period.lower)\n",
    "\n",
    "samples_with_time_period = samples_with_time_period.reset_index(drop=True)\n",
    "\n",
    "interpolated_proxies = {}\n",
    "\n",
    "for unique_lat in unique_lats:\n",
    "    \n",
    "    # get a single proxy\n",
    "    df_temp = samples_with_time_period[samples_with_time_period['Lat'] == unique_lat]\n",
    "\n",
    "    try:\n",
    "        # interpolate the dataset\n",
    "        interp = scipy.interpolate.interp1d(df_temp['lower'],\n",
    "                                        df_temp['d13C'],\n",
    "                                        bounds_error = False)\n",
    "    except ValueError:\n",
    "        interpolated_proxies.update({\n",
    "            unique_lat : df_temp\n",
    "        })\n",
    "    \n",
    "    single_proxy_interpolated = pd.DataFrame({'lower' : years_list, 'd13C' : interp(years_list)})\n",
    "    \n",
    "    location_repeated = pd.concat([df_temp.reset_index(0).loc[0,['Lat', 'Lon', 'Ocean_depth']]] * len(single_proxy_interpolated), axis=1).T\n",
    "    single_proxy_interpolated = pd.concat([location_repeated.reset_index(drop=True), single_proxy_interpolated.reset_index(drop=True)],axis=1)    \n",
    "    \n",
    "    interpolated_proxies.update({\n",
    "        unique_lat : single_proxy_interpolated\n",
    "    })\n",
    "    \n",
    "interpolated_samples = pd.concat(interpolated_proxies).reset_index(drop=True)\n",
    "interpolated_samples_backup = interpolated_samples.copy()\n",
    "\n",
    "# Combine the interpolated and not interpolated dataframes together so that\n",
    "\n",
    "interpolated_samples = pd.merge(\n",
    "    proxy_compilation, interpolated_samples, on=['Lat','Lon','Ocean_depth','d13C','lower'],\n",
    "    indicator=True,how='right'\n",
    ")\n",
    "\n",
    "# Replace the keys 'both' or 'right_only' with colors, to set scatter plot fill\n",
    "interpolated_samples.replace('right_only','none',inplace=True)\n",
    "\n",
    "shallower_samples = interpolated_samples[interpolated_samples['Ocean_depth'] < dividing_depth].replace('both',str(point_color_shallow))\n",
    "deeper_samples = interpolated_samples[interpolated_samples['Ocean_depth'] > dividing_depth].replace('both',str(point_color_deep))\n",
    "\n",
    "interpolated_samples = shallower_samples.append(deeper_samples)\n",
    "edgecolor = [point_color_deep if x >= dividing_depth else point_color_shallow for x in interpolated_samples.Ocean_depth]\n",
    "interpolated_samples['edgecolor'] = edgecolor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get volume weight average with time slices\n",
    "\n",
    "interpolated_samples['weights'] = np.nan\n",
    "\n",
    "# North\n",
    "interpolated_samples.loc[(interpolated_samples['Lat'] > 0) ,'weights'] = 21.2\n",
    "\n",
    "# South\n",
    "interpolated_samples.loc[(interpolated_samples['Lat'] < 0) ,'weights'] = 23.9\n",
    "\n",
    "# group the cores based on the age and the region (indicated by weights)\n",
    "grouped_by_age_region = interpolated_samples.groupby(['lower', 'weights'])\n",
    "\n",
    "averaged_by_age_region = {}\n",
    "\n",
    "# find group means\n",
    "for key, group in grouped_by_age_region:\n",
    "    averaged_by_age_region.update({\n",
    "        key: np.mean(group)\n",
    "    })\n",
    "    \n",
    "averaged_by_age_region = pd.concat(averaged_by_age_region,axis=1).T.reset_index(drop=True)\n",
    "\n",
    "# Now there is one values for each region (weight) and each year combination\n",
    "# Group by years and use weights to find the average d13C for that time period\n",
    "\n",
    "grouped_by_age = averaged_by_age_region.groupby('lower')\n",
    "\n",
    "averaged_by_age = {}\n",
    "stdev_by_age = {}\n",
    "\n",
    "# find group means\n",
    "for key, group in grouped_by_age:\n",
    "    \n",
    "    # Find the normal average\n",
    "    avg = np.mean(group.d13C)\n",
    "    \n",
    "    averaged_by_age.update({\n",
    "        np.mean(group['lower']) : np.sum(group['d13C'] * group['weights'])/np.sum(group['weights'])\n",
    "    })\n",
    "    stdev_by_age.update({\n",
    "        np.mean(group['lower']) : np.average((group.d13C-avg)**2,weights=group.weights)\n",
    "    }) \n",
    "    \n",
    "    \n",
    "averaged_by_age = pd.DataFrame.from_dict(averaged_by_age,orient='index').reset_index(drop=False).rename(columns={'index' : 'lower', 0 : 'd13C'})\n",
    "stdev_by_age = pd.DataFrame.from_dict(stdev_by_age,orient='index').reset_index(drop=False).rename(columns={'index' : 'lower', 0 : 'stdev'})\n",
    "\n",
    "averaged_by_age = pd.merge(left=averaged_by_age,right=stdev_by_age)\n",
    "averaged_by_age = averaged_by_age.sort_values('lower')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy the Holocene datasets to new variables\n",
    "averaged_by_age_LIG_Pacific = averaged_by_age.copy()\n",
    "interpolated_samples_LIG_Pacific = interpolated_samples.copy()\n",
    "\n",
    "interpolated_samples = interpolated_samples.drop('weights',axis=1)\n",
    "# Save the datasets\n",
    "averaged_by_age.to_csv(\"Data/Pacific_LIG_profile_PL.csv\", index=False)\n",
    "interpolated_samples.to_csv(\"/srv/ccrc/data06/z5145948/Moving_water_mass/Data/interpolated_LIG_Pacific.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holocene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ################## read in the oliver data using the predefined function\n",
    "\n",
    "df_pl = pl_cores(Hol_min,Hol_max)\n",
    "\n",
    "df_oliver = Oliver_cores(Hol_min,Hol_max)\n",
    "\n",
    "###################3 join the dataframes\n",
    "\n",
    "df_results = df_pl.append(df_oliver)\n",
    "  \n",
    "#################### get only the indian cores\n",
    "\n",
    "df = df_results.reset_index(drop = True)\n",
    "df['Dep'] = abs(df['Dep'])\n",
    "\n",
    "df_ind = df[df['Location'] == 'Indian']\n",
    "df_ind = df_ind.reset_index(drop = True)\n",
    "df_ind['age'] = df_ind.age.astype(float)\n",
    "\n",
    "# Slice the data\n",
    "lower = np.arange(Hol_min, Hol_max, slice_width)\n",
    "upper = np.arange(Hol_min+slice_width, Hol_max+slice_width, slice_width)\n",
    "\n",
    "proxy_compilation = {}\n",
    "\n",
    "for low, up in zip(lower, upper):\n",
    "    df_slice = df_ind[(df_ind['age'] >= low) & (df_ind['age'] < up)]\n",
    "    grouped_slice = df_slice.groupby(['Lat', 'Lon', 'Dep'])\n",
    "\n",
    "    slice_averaged = {}\n",
    "\n",
    "    for key, group in grouped_slice:\n",
    "        group_averaged = group.mean()\n",
    "        group_count = group.count()\n",
    "        group_averaged['count'] = group_count['d13C']\n",
    "        slice_averaged.update({\n",
    "            group.Core.reset_index(drop=True)[0] : pd.DataFrame(group_averaged)\n",
    "        })\n",
    "    \n",
    "    slice_averaged = pd.concat(slice_averaged, axis=1).T\n",
    "    slice_averaged = slice_averaged.drop(['age'],axis=1)\n",
    "    \n",
    "    proxy_compilation.update({\n",
    "        low : slice_averaged.T\n",
    "    })\n",
    "\n",
    "proxy_compilation = pd.concat(proxy_compilation,axis=1).T.reset_index(drop=False)\n",
    "proxy_compilation = proxy_compilation.drop('level_2',axis=1)\n",
    "proxy_compilation = proxy_compilation.rename(columns={'level_0' : 'lower', 'level_1' : 'Core', 'Dep' : 'Ocean_depth'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save list of cores to latex table to include in paper\n",
    "latex_table_Hol = proxy_compilation.drop(['d13C','count','lower'],axis=1)\n",
    "latex_table_Hol = latex_table_Hol.drop_duplicates()\n",
    "\n",
    "proxy_compilation.drop('Core',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# interpolating across the entire dataset\n",
    "\n",
    "# Add time bounds to the samples table\n",
    "samples_with_time_period = proxy_compilation\n",
    "\n",
    "unique_lats = np.unique(samples_with_time_period['Lat'])\n",
    "years_list = np.unique(samples_with_time_period.lower)\n",
    "\n",
    "samples_with_time_period = samples_with_time_period.reset_index(drop=True)\n",
    "\n",
    "interpolated_proxies = {}\n",
    "\n",
    "for unique_lat in unique_lats:\n",
    "    \n",
    "    # get a single proxy\n",
    "    df_temp = samples_with_time_period[samples_with_time_period['Lat'] == unique_lat]\n",
    "\n",
    "    try:\n",
    "        # interpolate the dataset\n",
    "        interp = scipy.interpolate.interp1d(df_temp['lower'],\n",
    "                                        df_temp['d13C'],\n",
    "                                        bounds_error = False)\n",
    "    except ValueError:\n",
    "        interpolated_proxies.update({\n",
    "            unique_lat : df_temp\n",
    "        })\n",
    "    \n",
    "    single_proxy_interpolated = pd.DataFrame({'lower' : years_list, 'd13C' : interp(years_list)})\n",
    "    \n",
    "    location_repeated = pd.concat([df_temp.reset_index(0).loc[0,['Lat', 'Lon', 'Ocean_depth']]] * len(single_proxy_interpolated), axis=1).T\n",
    "    single_proxy_interpolated = pd.concat([location_repeated.reset_index(drop=True), single_proxy_interpolated.reset_index(drop=True)],axis=1)    \n",
    "    \n",
    "    interpolated_proxies.update({\n",
    "        unique_lat : single_proxy_interpolated\n",
    "    })\n",
    "    \n",
    "interpolated_samples = pd.concat(interpolated_proxies).reset_index(drop=True)\n",
    "interpolated_samples_backup = interpolated_samples.copy()\n",
    "\n",
    "# Combine the interpolated and not interpolated dataframes together so that\n",
    "\n",
    "interpolated_samples = pd.merge(\n",
    "    proxy_compilation, interpolated_samples, on=['Lat','Lon','Ocean_depth','d13C','lower'],\n",
    "    indicator=True,how='right'\n",
    ")\n",
    "\n",
    "# Replace the keys 'both' or 'right_only' with colors, to set scatter plot fill\n",
    "interpolated_samples.replace('right_only','none',inplace=True)\n",
    "\n",
    "shallower_samples = interpolated_samples[interpolated_samples['Ocean_depth'] < dividing_depth].replace('both',str(point_color_shallow))\n",
    "deeper_samples = interpolated_samples[interpolated_samples['Ocean_depth'] > dividing_depth].replace('both',str(point_color_deep))\n",
    "\n",
    "interpolated_samples = shallower_samples.append(deeper_samples)\n",
    "edgecolor = [point_color_deep if x >= dividing_depth else point_color_shallow for x in interpolated_samples.Ocean_depth]\n",
    "interpolated_samples['edgecolor'] = edgecolor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get volume weight average with time slices # Use a single region for the Indian Ocean\n",
    "\n",
    "interpolated_samples['weights'] = 1\n",
    "\n",
    "# group the cores based on the age and the region (indicated by weights)\n",
    "grouped_by_age_region = interpolated_samples.groupby(['lower', 'weights'])\n",
    "\n",
    "averaged_by_age_region = {}\n",
    "\n",
    "# find group means\n",
    "for key, group in grouped_by_age_region:\n",
    "    averaged_by_age_region.update({\n",
    "        key: np.mean(group)\n",
    "    })\n",
    "    \n",
    "averaged_by_age_region = pd.concat(averaged_by_age_region,axis=1).T.reset_index(drop=True)\n",
    "\n",
    "# Now there is one values for each region (weight) and each year combination\n",
    "# Group by years and use weights to find the average d13C for that time period\n",
    "\n",
    "grouped_by_age = averaged_by_age_region.groupby('lower')\n",
    "\n",
    "averaged_by_age = {}\n",
    "stdev_by_age = {}\n",
    "\n",
    "# find group means\n",
    "for key, group in grouped_by_age:\n",
    "    \n",
    "    # Find the normal average\n",
    "    avg = np.mean(group.d13C)\n",
    "    \n",
    "    averaged_by_age.update({\n",
    "        np.mean(group['lower']) : np.sum(group['d13C'] * group['weights'])/np.sum(group['weights'])\n",
    "    })\n",
    "    stdev_by_age.update({\n",
    "        np.mean(group['lower']) : np.average((group.d13C-avg)**2,weights=group.weights)\n",
    "    }) \n",
    "    \n",
    "    \n",
    "averaged_by_age = pd.DataFrame.from_dict(averaged_by_age,orient='index').reset_index(drop=False).rename(columns={'index' : 'lower', 0 : 'd13C'})\n",
    "stdev_by_age = pd.DataFrame.from_dict(stdev_by_age,orient='index').reset_index(drop=False).rename(columns={'index' : 'lower', 0 : 'stdev'})\n",
    "\n",
    "averaged_by_age = pd.merge(left=averaged_by_age,right=stdev_by_age)\n",
    "averaged_by_age = averaged_by_age.sort_values('lower')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy the Holocene datasets to new variables\n",
    "averaged_by_age_Holocene_Indian = averaged_by_age.copy()\n",
    "interpolated_samples_Holocene_Indian = interpolated_samples.copy()\n",
    "\n",
    "interpolated_samples = interpolated_samples.drop('weights',axis=1)\n",
    "# Save the datasets\n",
    "averaged_by_age.to_csv(\"Data/Indian_Holocene_profile_PL.csv\", index=False)\n",
    "interpolated_samples.to_csv(\"/srv/ccrc/data06/z5145948/Moving_water_mass/Data/interpolated_Holocene_Indian.csv\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pl = pl_cores(LIG_min,LIG_max)\n",
    "\n",
    "df_oliver = Oliver_cores(LIG_min,LIG_max)\n",
    "\n",
    "###################3 join the dataframes\n",
    "\n",
    "df_results = df_pl.append(df_oliver)\n",
    "  \n",
    "#################### get only the Indian cores\n",
    "\n",
    "df = df_results.reset_index(drop = True)\n",
    "df['Dep'] = abs(df['Dep'])\n",
    "\n",
    "df_ind = df[df['Location'] == 'Indian']\n",
    "df_ind = df_ind.reset_index(drop = True)\n",
    "df_ind['age'] = df_ind.age.astype(float)\n",
    "\n",
    "# Slice the data\n",
    "lower = np.arange(LIG_min, LIG_max, slice_width)\n",
    "upper = np.arange(LIG_min+slice_width, LIG_max+slice_width, slice_width)\n",
    "\n",
    "proxy_compilation = {}\n",
    "\n",
    "for low, up in zip(lower, upper):\n",
    "    df_slice = df_ind[(df_ind['age'] >= low) & (df_ind['age'] < up)]\n",
    "    grouped_slice = df_slice.groupby(['Lat', 'Lon', 'Dep'])\n",
    "\n",
    "    slice_averaged = {}\n",
    "\n",
    "    for key, group in grouped_slice:\n",
    "        group_averaged = group.mean()\n",
    "        group_count = group.count()\n",
    "        group_averaged['count'] = group_count['d13C']\n",
    "        slice_averaged.update({\n",
    "            group.Core.reset_index(drop=True)[0] : pd.DataFrame(group_averaged)\n",
    "        })\n",
    "    \n",
    "    slice_averaged = pd.concat(slice_averaged, axis=1).T\n",
    "    slice_averaged = slice_averaged.drop(['age'],axis=1)\n",
    "    \n",
    "    proxy_compilation.update({\n",
    "        low : slice_averaged.T\n",
    "    })\n",
    "\n",
    "proxy_compilation = pd.concat(proxy_compilation,axis=1).T.reset_index(drop=False)\n",
    "proxy_compilation = proxy_compilation.drop('level_2',axis=1)\n",
    "proxy_compilation = proxy_compilation.rename(columns={'level_0' : 'lower', 'level_1' : 'Core', 'Dep' : 'Ocean_depth'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save list of cores to latex table to include in paper\n",
    "latex_table_LIG = proxy_compilation.drop(['d13C','count','lower'],axis=1)\n",
    "latex_table_LIG = latex_table_LIG.drop_duplicates()\n",
    "\n",
    "proxy_compilation.drop('Core',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine the two latex tables\n",
    "latex_table = latex_table_Hol.merge(latex_table_LIG,on=['Core','Lat','Lon','Ocean_depth'],how='outer',indicator=True)\n",
    "\n",
    "# Format the table\n",
    "latex_table.replace(to_replace='both',value='Holocene & LIG',inplace=True)\n",
    "latex_table.replace(to_replace='right_only',value='LIG',inplace=True)\n",
    "latex_table.replace(to_replace='left_only',value='Holocene',inplace=True)\n",
    "latex_table = latex_table.rename(columns={'Lat':'Latitude','Lon' : 'Longitude', 'Ocean_depth' : 'Depth (m)', '_merge':'Time Period'})\n",
    "latex_table['Latitude'] = [str(round(x, 2)) for x in latex_table.Latitude]\n",
    "latex_table['Longitude'] = [str(round(x, 2)) for x in latex_table.Longitude]\n",
    "latex_table.sort_values(by='Core',inplace=True)\n",
    "\n",
    "# Convert to string of latex markdown\n",
    "latex_string = latex_table.to_latex(index=False,longtable=True)\n",
    "\n",
    "# Reformat some parts of the latex table\n",
    "latex_string = latex_string.replace('\\\\toprule','')\n",
    "latex_string = latex_string.replace('\\\\midrule','')\n",
    "latex_string = latex_string.replace('\\\\bottomrule','')\n",
    "\n",
    "# Add caption to latex table\n",
    "caption = [\n",
    "    '\\caption{Latitude, Longitude, and depth (m) coordinates for the Indian benthic foraminifera',\n",
    "    '$\\delta^{13}$C cores. \\\\textit{Time Period} refers to whether this core had',\n",
    "    'Holocene, Last Interglacial, or Holocene and Last Interglacial data.}'\n",
    "]\n",
    "\n",
    "caption = ' '.join(caption)\n",
    "\n",
    "latex_string = latex_string.replace('\\\\end{longtable}',caption+'\\\\end{longtable}')\n",
    "\n",
    "# Write to a file\n",
    "file1 = open(\"Figures/Cores_tables_Indian.tex\",\"w\") \n",
    "file1.write(latex_string) \n",
    "file1.close() #to change file access modes \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# interpolating across the entire dataset\n",
    "\n",
    "# Add time bounds to the samples table\n",
    "samples_with_time_period = proxy_compilation\n",
    "\n",
    "unique_lats = np.unique(samples_with_time_period['Lat'])\n",
    "years_list = np.unique(samples_with_time_period.lower)\n",
    "\n",
    "samples_with_time_period = samples_with_time_period.reset_index(drop=True)\n",
    "\n",
    "interpolated_proxies = {}\n",
    "\n",
    "for unique_lat in unique_lats:\n",
    "    \n",
    "    # get a single proxy\n",
    "    df_temp = samples_with_time_period[samples_with_time_period['Lat'] == unique_lat]\n",
    "\n",
    "    try:\n",
    "        # interpolate the dataset\n",
    "        interp = scipy.interpolate.interp1d(df_temp['lower'],\n",
    "                                        df_temp['d13C'],\n",
    "                                        bounds_error = False)\n",
    "    except ValueError:\n",
    "        interpolated_proxies.update({\n",
    "            unique_lat : df_temp\n",
    "        })\n",
    "    \n",
    "    single_proxy_interpolated = pd.DataFrame({'lower' : years_list, 'd13C' : interp(years_list)})\n",
    "    \n",
    "    location_repeated = pd.concat([df_temp.reset_index(0).loc[0,['Lat', 'Lon', 'Ocean_depth']]] * len(single_proxy_interpolated), axis=1).T\n",
    "    single_proxy_interpolated = pd.concat([location_repeated.reset_index(drop=True), single_proxy_interpolated.reset_index(drop=True)],axis=1)    \n",
    "    \n",
    "    interpolated_proxies.update({\n",
    "        unique_lat : single_proxy_interpolated\n",
    "    })\n",
    "    \n",
    "interpolated_samples = pd.concat(interpolated_proxies).reset_index(drop=True)\n",
    "interpolated_samples_backup = interpolated_samples.copy()\n",
    "\n",
    "# Combine the interpolated and not interpolated dataframes together so that\n",
    "\n",
    "interpolated_samples = pd.merge(\n",
    "    proxy_compilation, interpolated_samples, on=['Lat','Lon','Ocean_depth','d13C','lower'],\n",
    "    indicator=True,how='right'\n",
    ")\n",
    "\n",
    "# Replace the keys 'both' or 'right_only' with colors, to set scatter plot fill\n",
    "interpolated_samples.replace('right_only','none',inplace=True)\n",
    "\n",
    "shallower_samples = interpolated_samples[interpolated_samples['Ocean_depth'] < dividing_depth].replace('both',str(point_color_shallow))\n",
    "deeper_samples = interpolated_samples[interpolated_samples['Ocean_depth'] > dividing_depth].replace('both',str(point_color_deep))\n",
    "\n",
    "interpolated_samples = shallower_samples.append(deeper_samples)\n",
    "edgecolor = [point_color_deep if x >= dividing_depth else point_color_shallow for x in interpolated_samples.Ocean_depth]\n",
    "interpolated_samples['edgecolor'] = edgecolor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get volume weight average with time slices\n",
    "\n",
    "interpolated_samples['weights'] = 1\n",
    "\n",
    "# group the cores based on the age and the region (indicated by weights)\n",
    "grouped_by_age_region = interpolated_samples.groupby(['lower', 'weights'])\n",
    "\n",
    "averaged_by_age_region = {}\n",
    "\n",
    "# find group means\n",
    "for key, group in grouped_by_age_region:\n",
    "    averaged_by_age_region.update({\n",
    "        key: np.mean(group)\n",
    "    })\n",
    "    \n",
    "averaged_by_age_region = pd.concat(averaged_by_age_region,axis=1).T.reset_index(drop=True)\n",
    "\n",
    "# Now there is one values for each region (weight) and each year combination\n",
    "# Group by years and use weights to find the average d13C for that time period\n",
    "\n",
    "grouped_by_age = averaged_by_age_region.groupby('lower')\n",
    "\n",
    "averaged_by_age = {}\n",
    "stdev_by_age = {}\n",
    "\n",
    "# find group means\n",
    "for key, group in grouped_by_age:\n",
    "    \n",
    "    # Find the normal average\n",
    "    avg = np.mean(group.d13C)\n",
    "    \n",
    "    averaged_by_age.update({\n",
    "        np.mean(group['lower']) : np.sum(group['d13C'] * group['weights'])/np.sum(group['weights'])\n",
    "    })\n",
    "    stdev_by_age.update({\n",
    "        np.mean(group['lower']) : np.average((group.d13C-avg)**2,weights=group.weights)\n",
    "    }) \n",
    "    \n",
    "    \n",
    "averaged_by_age = pd.DataFrame.from_dict(averaged_by_age,orient='index').reset_index(drop=False).rename(columns={'index' : 'lower', 0 : 'd13C'})\n",
    "stdev_by_age = pd.DataFrame.from_dict(stdev_by_age,orient='index').reset_index(drop=False).rename(columns={'index' : 'lower', 0 : 'stdev'})\n",
    "\n",
    "averaged_by_age = pd.merge(left=averaged_by_age,right=stdev_by_age)\n",
    "averaged_by_age = averaged_by_age.sort_values('lower')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy the Holocene datasets to new variables\n",
    "averaged_by_age_LIG_Indian = averaged_by_age.copy()\n",
    "interpolated_samples_LIG_Indian = interpolated_samples.copy()\n",
    "\n",
    "interpolated_samples = interpolated_samples.drop('weights',axis=1)\n",
    "# Save the datasets\n",
    "averaged_by_age.to_csv(\"Data/Indian_LIG_profile_PL.csv\", index=False)\n",
    "interpolated_samples.to_csv(\"/srv/ccrc/data06/z5145948/Moving_water_mass/Data/interpolated_LIG_Indian.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## holocene "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reformat individual ocean dataframes so that they can be combined into a single dataframe\n",
    "averaged_by_age_Holocene_Indian.rename(columns={'d13C':'d13C_ind','stdev':'stdev_ind'},inplace=True)\n",
    "averaged_by_age_Holocene_Indian.set_index('lower',inplace=True)\n",
    "averaged_by_age_Holocene_Pacific.rename(columns={'d13C':'d13C_pac','stdev':'stdev_pac'},inplace=True)\n",
    "averaged_by_age_Holocene_Pacific.set_index('lower',inplace=True)\n",
    "averaged_by_age_Holocene_Atlantic.rename(columns={'d13C':'d13C_atl','stdev':'stdev_atl'},inplace=True)\n",
    "averaged_by_age_Holocene_Atlantic.set_index('lower',inplace=True)\n",
    "averaged_by_age_LIG_Indian.rename(columns={'d13C':'d13C_ind','stdev':'stdev_ind'},inplace=True)\n",
    "averaged_by_age_LIG_Indian.set_index('lower',inplace=True)\n",
    "averaged_by_age_LIG_Pacific.rename(columns={'d13C':'d13C_pac','stdev':'stdev_pac'},inplace=True)\n",
    "averaged_by_age_LIG_Pacific.set_index('lower',inplace=True)\n",
    "averaged_by_age_LIG_Atlantic.rename(columns={'d13C':'d13C_atl','stdev':'stdev_atl'},inplace=True)\n",
    "averaged_by_age_LIG_Atlantic.set_index('lower',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge into a single data frame for each time period\n",
    "global_ocean_slices_Holocene = pd.concat([averaged_by_age_Holocene_Indian,averaged_by_age_Holocene_Pacific,averaged_by_age_Holocene_Atlantic],axis=1)\n",
    "global_ocean_slices_Holocene.reset_index(inplace=True)\n",
    "\n",
    "global_ocean_slices_LIG = pd.concat([averaged_by_age_LIG_Indian,averaged_by_age_LIG_Pacific,averaged_by_age_LIG_Atlantic],axis=1)\n",
    "global_ocean_slices_LIG.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate volume weighted standard deviations and average d13C\n",
    "global_ocean_slices_Holocene['d13C'] = (global_ocean_slices_Holocene['d13C_atl']*18.4 +\n",
    "                                   global_ocean_slices_Holocene['d13C_pac']*45.1 +\n",
    "                                   global_ocean_slices_Holocene['d13C_ind']*14.8\n",
    "\n",
    ")/(18.4+45.1+14.8)\n",
    "\n",
    "global_ocean_slices_Holocene['stdev'] = (global_ocean_slices_Holocene['stdev_atl']*18.4 +\n",
    "                                   global_ocean_slices_Holocene['stdev_pac']*45.1 +\n",
    "                                   global_ocean_slices_Holocene['stdev_ind']*14.8\n",
    "\n",
    ")/(18.4+45.1+14.8)\n",
    "\n",
    "global_ocean_slices_LIG['d13C'] = (global_ocean_slices_LIG['d13C_atl']*18.4 +\n",
    "                                   global_ocean_slices_LIG['d13C_pac']*45.1 +\n",
    "                                   global_ocean_slices_LIG['d13C_ind']*14.8\n",
    "\n",
    ")/(18.4+45.1+14.8)\n",
    "\n",
    "global_ocean_slices_LIG['stdev'] = (global_ocean_slices_LIG['stdev_atl']*18.4 +\n",
    "                                   global_ocean_slices_LIG['stdev_pac']*45.1 +\n",
    "                                   global_ocean_slices_LIG['stdev_ind']*14.8\n",
    "\n",
    ")/(18.4+45.1+14.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the datasets\n",
    "global_ocean_slices_Holocene.to_csv(\"Data/Global_Holocene_profile_PL.csv\", index=False)\n",
    "global_ocean_slices_LIG.to_csv(\"Data/Global_LIG_profile_PL.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
